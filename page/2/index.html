<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="skydh">
<meta property="og:type" content="website">
<meta property="og:title" content="learning, progress, future.">
<meta property="og:url" content="https://skydh.github.io/page/2/index.html">
<meta property="og:site_name" content="learning, progress, future.">
<meta property="og:description" content="skydh">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="learning, progress, future.">
<meta name="twitter:description" content="skydh">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://skydh.github.io/page/2/"/>





  <title>learning, progress, future.</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">learning, progress, future.</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">skydh</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://skydh.github.io/2019/10/10/spring @import/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="skydh">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="learning, progress, future.">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/10/spring @import/" itemprop="url">spring @import注解原理解析</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-10-10T16:20:38+08:00">
                2019-10-10
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="import"><a href="#import" class="headerlink" title="@import"></a>@import</h1><p>　　我们可以用这个注解导入bean到容器里面。</p>
<h1 id="3个方式"><a href="#3个方式" class="headerlink" title="3个方式"></a>3个方式</h1><h2 id="直接-import"><a href="#直接-import" class="headerlink" title="直接@import"></a>直接@import</h2><p>　　导入的baen</p>
<pre><code>@Data
public class User {
    private Integer id;
    private String name;
}
</code></pre><p>　　导入方法：</p>
<pre><code>@Configuration
@Import(value = { User.class })
public class Config {

}
</code></pre><h2 id="通过ImportBeanDefinitionRegistrar"><a href="#通过ImportBeanDefinitionRegistrar" class="headerlink" title="通过ImportBeanDefinitionRegistrar"></a>通过ImportBeanDefinitionRegistrar</h2><p>　　还是上面的bean。但是 不是直接导入。</p>
<pre><code>public class UserServiceBeanDefinitionRegistrar implements ImportBeanDefinitionRegistrar {

public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) {
    BeanDefinitionBuilder user = BeanDefinitionBuilder.rootBeanDefinition(User.class);
    // 通过registry就可以注入到容器里啦
    registry.registerBeanDefinition(&quot;user&quot;, user.getBeanDefinition());
}
}
</code></pre><p>　　注解方法：</p>
<pre><code>@Configuration
@Import(value = { UserServiceBeanDefinitionRegistrar.class })
public class Config {

}
</code></pre><h2 id="通过ImportSelector"><a href="#通过ImportSelector" class="headerlink" title="通过ImportSelector"></a>通过ImportSelector</h2><pre><code>public class UserServiceImportSelect implements ImportSelector {
public String[] selectImports(AnnotationMetadata importingClassMetadata) {
    return new String[] { User.class.getName() };
}
}
</code></pre><p>　　注解方法：</p>
<pre><code>@Configuration
@Import(value = { UserServiceImportSelect.class })
public class Config {

}
</code></pre><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>　　上述３个方法都可以将bean导入到springbean容器里面。</p>
<h2 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h2><p>　　老规矩，我们从springboot的启动开始分析，虽然这直接属于spring那一块。我们从run开始点点点，进入下面方法。</p>
<pre><code>public ConfigurableApplicationContext run(String... args) {
        ......
        refreshContext(context);
        ......
}
</code></pre><p>　　直接关注这个方法，这个方法会调用核心，他是spring容器启动的核心方法。<br>　　然后不断点点，进入到这个核心方法。</p>
<pre><code>    @Override
public void refresh() throws BeansException, IllegalStateException {
    synchronized (this.startupShutdownMonitor) {
        ．．．．．

        invokeBeanFactoryPostProcessors(beanFactory);

        ．．．．．


}
</code></pre><p>　　这个方法提取出invokeBeanFactoryPostProcessors方法，点进去。</p>
<pre><code>public static void invokeBeanFactoryPostProcessors(
        ConfigurableListableBeanFactory beanFactory, List&lt;BeanFactoryPostProcessor&gt; beanFactoryPostProcessors) {

......

List&lt;BeanFactoryPostProcessor&gt; priorityOrderedPostProcessors = new ArrayList&lt;&gt;();

......

invokeBeanFactoryPostProcessors(priorityOrderedPostProcessors, beanFactory);

......
}
</code></pre><p>　</p>
<p>　　进入这个方法后主要有２个方法需要注意。这个接口BeanFactoryPostProcessor的实现类是这个ConfigurationClassPostProcessor。进入invokeBeanFactoryPostProcessors方法。</p>
<pre><code>private static void invokeBeanFactoryPostProcessors(
        Collection&lt;? extends BeanFactoryPostProcessor&gt; postProcessors, ConfigurableListableBeanFactory beanFactory) {

    for (BeanFactoryPostProcessor postProcessor : postProcessors) {
        postProcessor.postProcessBeanFactory(beanFactory);
    }
}
</code></pre><p>　　调用了postProcessBeanFactory方法。然后点点点，进入processConfigBeanDefinitions方法，这个就是处理@import的方法了。</p>
<pre><code>    public void processConfigBeanDefinitions(BeanDefinitionRegistry registry) {
......
parser.parse(candidates);
......

}
</code></pre><p>　　进入这个方法，不断进入doProcessConfigurationClass这个方法里面。</p>
<pre><code>@Nullable
protected final SourceClass doProcessConfigurationClass(ConfigurationClass configClass, SourceClass sourceClass)
        throws IOException {

    ......
    Set&lt;BeanDefinitionHolder&gt; scannedBeanDefinitions =
                    this.componentScanParser.parse(componentScan, sourceClass.getMetadata().getClassName());
            // Check the set of scanned definitions for any further config classes and parse recursively if needed
            for (BeanDefinitionHolder holder : scannedBeanDefinitions) {
                BeanDefinition bdCand = holder.getBeanDefinition().getOriginatingBeanDefinition();
                if (bdCand == null) {
                    bdCand = holder.getBeanDefinition();
                }
                if (ConfigurationClassUtils.checkConfigurationClassCandidate(bdCand, this.metadataReaderFactory)) {
                    parse(bdCand.getBeanClassName(), holder.getBeanName());
                }
            }
        }
    }

    // Process any @Import annotations
    processImports(configClass, sourceClass, getImports(sourceClass), true); 

    ......
    }
</code></pre><p>　　进去后，第一个方法获取到本项目下所有的bean,然后过滤找到是@Import的bean,再然后在processImports方法里面将其里面的bean注入spring容器进去。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://skydh.github.io/2019/10/09/多cookie导致的登录问题/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="skydh">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="learning, progress, future.">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/09/多cookie导致的登录问题/" itemprop="url">多cookie导致的登录问题</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-10-09T14:05:23+08:00">
                2019-10-09
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="问题如下"><a href="#问题如下" class="headerlink" title="问题如下"></a>问题如下</h1><p>　　本系统登录用户中心后，用户中心返回一个cookie给浏览器，浏览器携带cookie访问我们后台服务，后台服务通过cookie，从redis里面获取到信息后，向用户中心发送rpc请求，从而进行权限校验。问题如下，用户中心修改配置后，账号登录变成单端登录，A登录a账号，B登录a账号，B把A挤了下来。但是A不知道，继续访问后台服务，报错，没有权限，但是浏览器端有了新的cookie。再次登录，用户中心报错。</p>
<h1 id="分析如下"><a href="#分析如下" class="headerlink" title="分析如下"></a>分析如下</h1><p>　　２个方案，用户中心分析这个cookie，选择正确的cookie处理，或者本系统在那个情况下不生成这个cookie。</p>
<h1 id="源码解析"><a href="#源码解析" class="headerlink" title="源码解析"></a>源码解析</h1><p>　　springmvc 请求一个服务，要经过一系列的过滤器链，而我们使用了springsession,那么其中就有一个优先级很高的一个过滤器SessionRepositoryFilter。核心方法为</p>
<pre><code>@Override
protected void doFilterInternal(HttpServletRequest request,
        HttpServletResponse response, FilterChain filterChain)
        throws ServletException, IOException {
    request.setAttribute(SESSION_REPOSITORY_ATTR, this.sessionRepository);

    SessionRepositoryRequestWrapper wrappedRequest = new SessionRepositoryRequestWrapper(
            request, response, this.servletContext);
    SessionRepositoryResponseWrapper wrappedResponse = new SessionRepositoryResponseWrapper(
            wrappedRequest, response);

    try {
        filterChain.doFilter(wrappedRequest, wrappedResponse);
    }
    finally {
        wrappedRequest.commitSession();
    }
}
</code></pre><p>　　我们注意到，无论如何都要执行的finally代码块。进去观察</p>
<pre><code>private void commitSession() {
        HttpSessionWrapper wrappedSession = getCurrentSession();
        if (wrappedSession == null) {
            if (isInvalidateClientSession()) {
                SessionRepositoryFilter.this.httpSessionIdResolver.expireSession(this,
                        this.response);
            }
        }
        else {
            S session = wrappedSession.getSession();
            clearRequestedSessionCache();
            SessionRepositoryFilter.this.sessionRepository.save(session);
            String sessionId = session.getId();
            if (!isRequestedSessionIdValid()
                    || !sessionId.equals(getRequestedSessionId())) {
                SessionRepositoryFilter.this.httpSessionIdResolver.setSessionId(this,
                        this.response, sessionId);
            }
        }
    }
</code></pre><p>　　最后一行代码会进入CookieHttpSessionIdResolver的setSessionId方法中，我们观察这个方法。</p>
<pre><code>@Override
public void setSessionId(HttpServletRequest request, HttpServletResponse response,
        String sessionId) {
    if (sessionId.equals(request.getAttribute(WRITTEN_SESSION_ID_ATTR))) {
        return;
    }
    request.setAttribute(WRITTEN_SESSION_ID_ATTR, sessionId);
    this.cookieSerializer
            .writeCookieValue(new CookieValue(request, response, sessionId));
}
</code></pre><p>　　其会进入DefaultCookieSerializer的writeCookieValue方法里面</p>
<pre><code>@Override
public void writeCookieValue(CookieValue cookieValue) {
    HttpServletRequest request = cookieValue.getRequest();
    HttpServletResponse response = cookieValue.getResponse();

    StringBuilder sb = new StringBuilder();
    sb.append(this.cookieName).append(&apos;=&apos;);
    String value = getValue(cookieValue);
    if (value != null &amp;&amp; value.length() &gt; 0) {
        validateValue(value);
        sb.append(value);
    }
    int maxAge = getMaxAge(cookieValue);
    if (maxAge &gt; -1) {
        sb.append(&quot;; Max-Age=&quot;).append(cookieValue.getCookieMaxAge());
        OffsetDateTime expires = (maxAge != 0)
                ? OffsetDateTime.now().plusSeconds(maxAge)
                : Instant.EPOCH.atOffset(ZoneOffset.UTC);
        sb.append(&quot;; Expires=&quot;)
                .append(expires.format(DateTimeFormatter.RFC_1123_DATE_TIME));
    }
    String domain = getDomainName(request);
    if (domain != null &amp;&amp; domain.length() &gt; 0) {
        validateDomain(domain);
        sb.append(&quot;; Domain=&quot;).append(domain);
    }
    String path = getCookiePath(request);
    if (path != null &amp;&amp; path.length() &gt; 0) {
        validatePath(path);
        sb.append(&quot;; Path=&quot;).append(path);
    }
    if (isSecureCookie(request)) {
        sb.append(&quot;; Secure&quot;);
    }
    if (this.useHttpOnlyCookie) {
        sb.append(&quot;; HttpOnly&quot;);
    }
    if (this.sameSite != null) {
        sb.append(&quot;; SameSite=&quot;).append(this.sameSite);
    }

    response.addHeader(&quot;Set-Cookie&quot;, sb.toString());
}
</code></pre><p>　　找到了，原来在这里会在response里面写入cookie，在这个方法里面只要我们指定了cookie的name和domain和用户中心传给我们的一样，那么就会解决多cookie问题。</p>
<p>　　从系统启动开始看，加了EnableRedisHttpSession这个注解，会采用spring session，点进去里面有个@import注解，引入了RedisHttpSessionConfiguration这个配置类，这个类里面配置引入了很多bean。同时继承了一个SpringHttpSessionConfiguration这个抽象类，在这个抽象类里面有2个方法和我们强相关</p>
<pre><code>@Autowired(required = false)
public void setCookieSerializer(CookieSerializer cookieSerializer) {
    this.cookieSerializer = cookieSerializer;
}

@PostConstruct
public void init() {
    CookieSerializer cookieSerializer = (this.cookieSerializer != null)
            ? this.cookieSerializer
            : createDefaultCookieSerializer();
    this.defaultHttpSessionIdResolver.setCookieSerializer(cookieSerializer);
}
</code></pre><p>　　第一个方法是当容器中存在CookieSerializer实例时，就注入到这个bean中，第二个方法是这个bean执行完构造方法和所有注入后执行的方法，这个方法里面的逻辑如下，如果上下文不存在CookieSerializer这个实例，那么直接new一个。</p>
<p>　　那么解决方案就出来。我们可以创建一个bean注入到spring容器里面即可</p>
<pre><code>@Configuration
public class CookieConfiguration {
@Value(&quot;${cookie.name}&quot;)
private String cookieName;

@Value(&quot;${cookie.domain}&quot;)
private String domain;

@Bean
public DefaultCookieSerializer defaultCookieSerializer() {
    DefaultCookieSerializer serializer = new DefaultCookieSerializer();
    serializer.setCookieName(cookieName);
    serializer.setDomainName(domain);
    return serializer;
}
}
</code></pre><p>　　如此即可，返回时写cookie时，name和domain就指定了，就会覆盖之前的cookie，不会出现2个cookie了。问题解决。</p>
<h1 id="什么情况下会写入cookie。"><a href="#什么情况下会写入cookie。" class="headerlink" title="什么情况下会写入cookie。"></a>什么情况下会写入cookie。</h1><p>　　再往上走，会发现</p>
<pre><code>if (!isRequestedSessionIdValid()
                    || !sessionId.equals(getRequestedSessionId())) 
</code></pre><p>　　这个条件下，才会执行写入cookie的情况。我们的上述情况一定在这个里面。主要是第一个条件我们看看，点进去看看</p>
<pre><code>@Override
    public boolean isRequestedSessionIdValid() {
        if (this.requestedSessionIdValid == null) {
            S requestedSession = getRequestedSession();
            if (requestedSession != null) {
                requestedSession.setLastAccessedTime(Instant.now());
            }
            return isRequestedSessionIdValid(requestedSession);
        }
        return this.requestedSessionIdValid;
    }
</code></pre><p>　　这个方法主要和requestedSessionIdValid这个值相关。我们通过eclipse的call功能，找到了只有这个方法才对这个属性做了修改。</p>
<pre><code>@Override
    public HttpSessionWrapper getSession(boolean create) {
        HttpSessionWrapper currentSession = getCurrentSession();
        if (currentSession != null) {
            return currentSession;
        }
        S requestedSession = getRequestedSession();
        if (requestedSession != null) {
            if (getAttribute(INVALID_SESSION_ID_ATTR) == null) {
                requestedSession.setLastAccessedTime(Instant.now());
                this.requestedSessionIdValid = true;
                currentSession = new HttpSessionWrapper(requestedSession, getServletContext());
                currentSession.setNew(false);
                setCurrentSession(currentSession);
                return currentSession;
            }
        }
        ........
    }
</code></pre><p>　　我省略了一部分代码。他会根据requestedSession来判断是否设值进去，因此进入方法getRequestedSession。</p>
<pre><code>private S getRequestedSession() {
        if (!this.requestedSessionCached) {
            List&lt;String&gt; sessionIds = SessionRepositoryFilter.this.httpSessionIdResolver
                    .resolveSessionIds(this);
            for (String sessionId : sessionIds) {
                if (this.requestedSessionId == null) {
                    this.requestedSessionId = sessionId;
                }
                S session = SessionRepositoryFilter.this.sessionRepository
                        .findById(sessionId);
                if (session != null) {
                    this.requestedSession = session;
                    this.requestedSessionId = sessionId;
                    break;
                }
            }
            this.requestedSessionCached = true;
        }
        return this.requestedSession;
    }
</code></pre><p>　　这里我就不再进入方法内部了，我就直接说这些方法的功能。先获取事先约定好的cookie的value值，然后通过这个value值从redis里面获取对应的hash值，然后组装成session返回给方法，如果不存在，则返回null。最后若是session不存在，那么就会创建一个丢到redis里面去。并且丢到浏览器前端里面。</p>
<p>　　因此当这个cookie过期，或者cookie在redis里面不存在时就会写入cookie。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://skydh.github.io/2019/09/23/springboot 选择日志/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="skydh">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="learning, progress, future.">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/23/springboot 选择日志/" itemprop="url">springboot 选择日志</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-23T17:55:12+08:00">
                2019-09-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="明确的概念"><a href="#明确的概念" class="headerlink" title="明确的概念"></a>明确的概念</h1><p>　　目前主流的日志框架是logback,log4j,javalogging,这是目前主流的日志框架，其中出了个适配器，相当于接口，slf4j的作者和logback,log4j是一个人，这个接口可以适配logback,log4j</p>
<h1 id="springboot启动时如何选择日志的呢"><a href="#springboot启动时如何选择日志的呢" class="headerlink" title="springboot启动时如何选择日志的呢"></a>springboot启动时如何选择日志的呢</h1><p>　　我们从springboot启动<br>　　预先知识：</p>
<ul>
<li>@Import,这个注解的value是Class类型的数组，我们可以直接传递一些类进去，那么这些类就会被导入到当前ioc容器。或者导入，这些类里面加了@Bean方法返回的对象。同时会将实现ImportSelector接口的selectImports方法返回的类的全限定名导入到ioc容器。</li>
</ul>
<p>　　流程如下：</p>
<p>　　１．启动类上面的@SpringBootApplication注解，点进去，再进去EnableAutoConfiguration注解，上面有个import注解，前面说过了，点进去这个AutoConfigurationImportSelector类，看到selectImports方法：</p>
<pre><code>@Override
public String[] selectImports(AnnotationMetadata annotationMetadata) {
    if (!isEnabled(annotationMetadata)) {
        return NO_IMPORTS;
    }
    AutoConfigurationMetadata autoConfigurationMetadata = AutoConfigurationMetadataLoader
            .loadMetadata(this.beanClassLoader);
    AutoConfigurationEntry autoConfigurationEntry = getAutoConfigurationEntry(
            autoConfigurationMetadata, annotationMetadata);
    return StringUtils.toStringArray(autoConfigurationEntry.getConfigurations());
}
</code></pre><p>　　我们看到第２行代码，点进去</p>
<pre><code>protected AutoConfigurationEntry getAutoConfigurationEntry(
        AutoConfigurationMetadata autoConfigurationMetadata,
        AnnotationMetadata annotationMetadata) {
    if (!isEnabled(annotationMetadata)) {
        return EMPTY_ENTRY;
    }
    AnnotationAttributes attributes = getAttributes(annotationMetadata);
    List&lt;String&gt; configurations = getCandidateConfigurations(annotationMetadata,
            attributes);
    configurations = removeDuplicates(configurations);
    Set&lt;String&gt; exclusions = getExclusions(annotationMetadata, attributes);
    checkExcludedClasses(configurations, exclusions);
    configurations.removeAll(exclusions);
    configurations = filter(configurations, autoConfigurationMetadata);
    fireAutoConfigurationImportEvents(configurations, exclusions);
    return new AutoConfigurationEntry(configurations, exclusions);
}
</code></pre><p>　　我们点到第五行代码点进去</p>
<pre><code>    protected List&lt;String&gt; getCandidateConfigurations(AnnotationMetadata metadata,
        AnnotationAttributes attributes) {
    List&lt;String&gt; configurations = SpringFactoriesLoader.loadFactoryNames(
            getSpringFactoriesLoaderFactoryClass(), getBeanClassLoader());
    Assert.notEmpty(configurations,
            &quot;No auto configuration classes found in META-INF/spring.factories. If you &quot;
                    + &quot;are using a custom packaging, make sure that file is correct.&quot;);
    return configurations;
}
</code></pre><p>　　不断深入SpringFactoriesLoader.loadFactoryNames方法</p>
<pre><code>private static Map&lt;String, List&lt;String&gt;&gt; loadSpringFactories(@Nullable ClassLoader classLoader) {
    MultiValueMap&lt;String, String&gt; result = cache.get(classLoader);
    if (result != null) {
        return result;
    }

    try {
        Enumeration&lt;URL&gt; urls = (classLoader != null ?
                classLoader.getResources(FACTORIES_RESOURCE_LOCATION) :
                ClassLoader.getSystemResources(FACTORIES_RESOURCE_LOCATION));
        result = new LinkedMultiValueMap&lt;&gt;();
        while (urls.hasMoreElements()) {
            URL url = urls.nextElement();
            UrlResource resource = new UrlResource(url);
            Properties properties = PropertiesLoaderUtils.loadProperties(resource);
            for (Map.Entry&lt;?, ?&gt; entry : properties.entrySet()) {
                String factoryClassName = ((String) entry.getKey()).trim();
                for (String factoryName : StringUtils.commaDelimitedListToStringArray((String) entry.getValue())) {
                    result.add(factoryClassName, factoryName.trim());
                }
            }
        }
        cache.put(classLoader, result);
        return result;
    }
    catch (IOException ex) {
        throw new IllegalArgumentException(&quot;Unable to load factories from location [&quot; +
                FACTORIES_RESOURCE_LOCATION + &quot;]&quot;, ex);
    }
}
</code></pre><p>  其中FACTORIES_RESOURCE_LOCATION= “META-INF/spring.factories”<br>　也就是说从当前jar将前面说的key为org.springframework.boot.autoconfigure.EnableAutoConfiguration的value全部拉出来然后反射实例化后注入到spring ioc容器。这些key都是自动化配置的key。再看看后面的步骤。</p>
<p>　　２。从SpringApplication.run(WebApplication.class, args)<br>进入到    </p>
<pre><code> */
public static ConfigurableApplicationContext run(Class&lt;?&gt;[] primarySources,
        String[] args) {
    return new SpringApplication(primarySources).run(args);
}
</code></pre><p>　　<br>　　这个方法里面创建了SpringApplication对象，进入到其构造方法。</p>
<pre><code>@SuppressWarnings({ &quot;unchecked&quot;, &quot;rawtypes&quot; })
public SpringApplication(ResourceLoader resourceLoader, Class&lt;?&gt;... primarySources) {
    this.resourceLoader = resourceLoader;
    Assert.notNull(primarySources, &quot;PrimarySources must not be null&quot;);
    this.primarySources = new LinkedHashSet&lt;&gt;(Arrays.asList(primarySources));
    this.webApplicationType = WebApplicationType.deduceFromClasspath();
    setInitializers((Collection) getSpringFactoriesInstances(
            ApplicationContextInitializer.class));
    setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class));
    this.mainApplicationClass = deduceMainApplicationClass();
}
</code></pre><p>　　在setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class));这行代码里面Class类型是ApplicationListener。这个方法前文讲述过了。这里特别提一下其中有个Listener和本文相关，那就是LoggingApplicationListener这个Listener。然后将其注入到SpringApplication的listeners属性里面。前面方法里面创建对象后立马调用了run方法。进去，这个run方法我就不细说了，都知道。</p>
<pre><code>try {
        listeners.running(context);
    }
</code></pre><p>　　这行代码最终是启动listeners的，点进去源码可以看到</p>
<pre><code>@Override
public void running(ConfigurableApplicationContext context) {
    context.publishEvent(
            new ApplicationReadyEvent(this.application, this.args, context));
}
</code></pre><p>　　然后我们进入到LoggingApplicationListener这个类的关键代码处</p>
<pre><code>@Override
public void onApplicationEvent(ApplicationEvent event) {
    if (event instanceof ApplicationStartingEvent) {
        onApplicationStartingEvent((ApplicationStartingEvent) event);
    }
    else if (event instanceof ApplicationEnvironmentPreparedEvent) {
        onApplicationEnvironmentPreparedEvent(
                (ApplicationEnvironmentPreparedEvent) event);
    }
    else if (event instanceof ApplicationPreparedEvent) {
        onApplicationPreparedEvent((ApplicationPreparedEvent) event);
    }
    else if (event instanceof ContextClosedEvent &amp;&amp; ((ContextClosedEvent) event)
            .getApplicationContext().getParent() == null) {
        onContextClosedEvent();
    }
    else if (event instanceof ApplicationFailedEvent) {
        onApplicationFailedEvent();
    }
}
</code></pre><p>  执行第一个事件。</p>
<pre><code>private void onApplicationStartingEvent(ApplicationStartingEvent event) {
    this.loggingSystem = LoggingSystem
            .get(event.getSpringApplication().getClassLoader());
    this.loggingSystem.beforeInitialize();
}
</code></pre><p>　　进入LoggingSystem的get方法里面。</p>
<pre><code>public static LoggingSystem get(ClassLoader classLoader) {
    String loggingSystem = System.getProperty(SYSTEM_PROPERTY);
    if (StringUtils.hasLength(loggingSystem)) {
        if (NONE.equals(loggingSystem)) {
            return new NoOpLoggingSystem();
        }
        return get(classLoader, loggingSystem);
    }
    return SYSTEMS.entrySet().stream()
            .filter((entry) -&gt; ClassUtils.isPresent(entry.getKey(), classLoader))
            .map((entry) -&gt; get(classLoader, entry.getValue())).findFirst()
            .orElseThrow(() -&gt; new IllegalStateException(
                    &quot;No suitable logging system located&quot;));
}
</code></pre><p>　　核心出来了在这里，从SYSTEMS这个map里面按顺序取值，而这个map的值是下面的。测试了下。顺序取出的顺序是logback,log4j,javalog。按照上面代码的逻辑，如果上下文存在这个日志类，那么springboot就使用哪个类。显然是优先使用logback</p>
<pre><code>private static final Map&lt;String, String&gt; SYSTEMS;

static {
    Map&lt;String, String&gt; systems = new LinkedHashMap&lt;&gt;();
    systems.put(&quot;ch.qos.logback.core.Appender&quot;,
            &quot;org.springframework.boot.logging.logback.LogbackLoggingSystem&quot;);
    systems.put(&quot;org.apache.logging.log4j.core.impl.Log4jContextFactory&quot;,
            &quot;org.springframework.boot.logging.log4j2.Log4J2LoggingSystem&quot;);
    systems.put(&quot;java.util.logging.LogManager&quot;,
            &quot;org.springframework.boot.logging.java.JavaLoggingSystem&quot;);
    SYSTEMS = Collections.unmodifiableMap(systems);
}
</code></pre><p>　　
　　</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://skydh.github.io/2019/09/06/java位运算/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="skydh">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="learning, progress, future.">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/06/java位运算/" itemprop="url">java 位运算</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-06T09:33:20+08:00">
                2019-09-06
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>ps： 在java中，负数的二进制为，正数的二进制的反码+1。</p>
<h1 id="左移-lt-lt"><a href="#左移-lt-lt" class="headerlink" title="左移( &lt;&lt; )"></a>左移( &lt;&lt; )</h1><p>　　把数字向左移动，比如 &lt;&lt;2，相当于向左移动2位，相当于做了乘法，乘积为2^2。符号位不变。低数位全部用０来补</p>
<pre><code>public static void main(String[] arg) {
    System.out.println(5 &lt;&lt; 2);
}
</code></pre><p>　　<br>　　输出为20,相当于5*4=20</p>
<h1 id="右移-gt-gt"><a href="#右移-gt-gt" class="headerlink" title="右移( &gt;&gt; )"></a>右移( &gt;&gt; )</h1><p>　　把数字向右移动，比如  &gt;&gt;2，相当于向左移动2位，相当于做了除法，除了2^2。符号位不变。正数用０补位，负数用１补位。</p>
<pre><code>public static void main(String[] arg) {
    System.out.println(5 &gt;&gt; 2);
}
</code></pre><p>　　输出为１</p>
<h1 id="无符号右移-gt-gt-gt"><a href="#无符号右移-gt-gt-gt" class="headerlink" title="无符号右移( &gt;&gt;&gt; )"></a>无符号右移( &gt;&gt;&gt; )</h1><p>ps：没有无符号左移<br>　　把数字向右移动，比如  &gt;&gt;2，相当于向左移动2位，相当于做了除法，除了2^2。符号位也变。正数没有啥变化，负数的话，变化蛮大的。因为高数位全部都是０来补充。</p>
<h2 id="位与-amp"><a href="#位与-amp" class="headerlink" title="位与( &amp; )"></a>位与( &amp; )</h2><p>　　将数字转换为２进制后做与运算，每一个数字都要做与运算。２个数进行位与时，将２个数均转换为２进制数，然后按照顺序每一位开始做对应的与运算。均为１才是１，否则，为０.</p>
<h2 id="位或"><a href="#位或" class="headerlink" title="位或( | )"></a>位或( | )</h2><p>　　将数字转换为２进制后做或运算，每一个数字都要做或运算。２个数进行位或时，将２个数均转换为２进制数，然后按照顺序每一位开始做对应的位或运算。均为０才是０，否则为１.</p>
<h2 id="位非"><a href="#位非" class="headerlink" title="位非( ~ )"></a>位非( ~ )</h2><p>　　将数字转换为２进制后做非运算，每一个数字都要做非运算。对这个数字每一位都要进行非运算，也就是取反。</p>
<h2 id="位异或"><a href="#位异或" class="headerlink" title="位异或( ^ )"></a>位异或( ^ )</h2><p>　　将数字转换为２进制后做异或运算，每一个数字都要做异或运算。２个数进行异或时，将２个数均转换为２进制数，然后按照顺序每一位开始做对应的异或运算。均为０，或者均为１，才是0，否则为０.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://skydh.github.io/2019/07/08/网关/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="skydh">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="learning, progress, future.">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/08/网关/" itemprop="url">Api网关</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-08T09:25:25+08:00">
                2019-07-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="什么是网关"><a href="#什么是网关" class="headerlink" title="什么是网关"></a>什么是网关</h2><p>　　网关（英语：Gateway）是转发其他服务器通信数据的服务器，接收从客户端发送来的请求时，它就像自己拥有资源的源服务器一样对请求进行处理。有时客户端可能都不会察觉，自己的通信目标是一个网关。</p>
<h2 id="api网关"><a href="#api网关" class="headerlink" title="api网关"></a>api网关</h2><p>　　API 网关将各系统对外暴露的服务聚合起来，所有要调用这些服务的系统都需要通过 API 网关进行访问，基于这种方式网关可以对 API 进行统一管控，例如：认证、鉴权、流量控制、协议转换、监控等等。</p>
<h3 id="面向-Web-或者移动-App"><a href="#面向-Web-或者移动-App" class="headerlink" title="面向 Web 或者移动 App"></a>面向 Web 或者移动 App</h3><p>　　这类场景，在物理形态上类似前后端分离，前端应用通过 API 调用后端服务，需要网关具有认证、鉴权、缓存、服务编排、监控告警等功能。</p>
<h3 id="面向合作伙伴开放-API"><a href="#面向合作伙伴开放-API" class="headerlink" title="面向合作伙伴开放 API"></a>面向合作伙伴开放 API</h3><p>　　这类场景，主要为了满足业务形态对外开放，与企业外部合作伙伴建立生态圈，此时的 API 网关注重安全认证、权限分级、流量管控、缓存等功能的建设。</p>
<h3 id="企业内部系统互联互通"><a href="#企业内部系统互联互通" class="headerlink" title="企业内部系统互联互通"></a>企业内部系统互联互通</h3><p>　　对于中大型的企业内部往往有几十、甚至上百个系统，尤其是微服务架构的兴起系统数量更是急剧增加。系统之间相互依赖，逐渐形成网状调用关系不便于管理和维护，需要 API 网关进行统一的认证、鉴权、流量管控、超时熔断、监控告警管理，从而提高系统的稳定性、降低重复建设、运维管理等成本。</p>
<h3 id="具备的能力"><a href="#具备的能力" class="headerlink" title="具备的能力"></a>具备的能力</h3><h4 id="服务注册和服务接入能力"><a href="#服务注册和服务接入能力" class="headerlink" title="服务注册和服务接入能力"></a>服务注册和服务接入能力</h4><h4 id="网关接入和发布核心功能"><a href="#网关接入和发布核心功能" class="headerlink" title="网关接入和发布核心功能"></a>网关接入和发布核心功能</h4><h4 id="服务安全"><a href="#服务安全" class="headerlink" title="服务安全"></a>服务安全</h4><h4 id="服务管控和治理"><a href="#服务管控和治理" class="headerlink" title="服务管控和治理"></a>服务管控和治理</h4><p>ps:<a href="https://www.infoq.cn/article/api-gateway-architecture-design" target="_blank" rel="noopener">https://www.infoq.cn/article/api-gateway-architecture-design</a></p>
<h2 id="zuul"><a href="#zuul" class="headerlink" title="zuul"></a>zuul</h2><p>　　我司的api网关服务是基于zuul2实现的。为了搭建一个类似的网关服务，我将详细的搭建一个demo，来跑这个服务。</p>
<p>　　我先根据网上的教程，搭建了一个简单的eureka注册中心，然后搭建了2个简单的server注册到这个注册中心里面，然后搭建一个简单的基于zuul的网关注册到eureka里面，这个api网关是基于serverId实现的，然后启动一个server，路由访问，成功得到数据。实现这个ZuulFilter这个接口做的效果，默认则是org.springframework.cloud.netflix.zuul.filters这个包下的过滤器的实现。</p>
<p>　　ps：参考这篇文章</p>
<pre><code>http://blog.didispace.com/spring-cloud-source-zuul/
</code></pre><p>　　</p>
<p>ps(SpringCloud的整体组建包括：Zuul、Ribbon、EureKa、Fein、Hystrix等。其中Zuul就是一个类似APIGateway的组建，Ribbon是类似于Nginx的代理服务器，Eureka用于注册和发现服务，Hystrix可以作为整个架构的断路服务，用于服务降级。Fein可以作为一个Rest服务的提供者，可以供内部服务之间相互调用)</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://skydh.github.io/2019/06/27/kafka基础-主题和分区/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="skydh">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="learning, progress, future.">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/27/kafka基础-主题和分区/" itemprop="url">kafka入门005 -主题和分区</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-27T20:40:55+08:00">
                2019-06-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="脚本管理主题信息"><a href="#脚本管理主题信息" class="headerlink" title="脚本管理主题信息"></a>脚本管理主题信息</h1><h2 id="创建主题"><a href="#创建主题" class="headerlink" title="创建主题"></a>创建主题</h2><p>　　在集群配置的时候就说过了　auto.create.topics.enable这个值设置为true时。生产者发送一个没有被创建主题的消息时，会自动创建一个分区数为num.partition默认为1，副本因子为default.replication.factory默认为1的主题。消费者请求某个主题时也会创建一个类似的主题。这样主题不利于管理，因此我们需要将　auto.create.topics.enable设置为false。<br>　　<br>　　我们一般通过这个命令在服务器通过kafka-topics.sh脚本来创建主题。</p>
<pre><code>./bin/kafka-topics.sh --zookeeper localhost:2181/kafka --create --topic topic-dh --partitions 4 --replication-factor 1
</code></pre><p>　　这个命令是创建一个叫做topic-dh的主题，分区有4个，副本因子2个。<br>　　配置文件里面有个logs的配置，里面配置了主题和分区。前面也说过了，每个副本必须在不同的broke。我们不仅可以通过刚刚的路径查看log日志文件，也可以通过。<br>　　我们不仅可以通过上述的日志文件查看主题，我们还可以通过zk客户端查看，每创建一个主题就会在zk的/brokers/topics上创建一个同名的实节点。该节点记录了创建该主题分区的分配方案。首先启动这个zk客户端，启动命令如下：<br>　　./zkCli.sh –server 127.0.0.1:2181。<br>　　然后在命令行界面输入获取节点。和书上不一样的是，我的节点信息存放在 /kafka/brokers/topics/topic-demo111。这个目录下面，没有存放在 /brokers/topics/topic-demo111里面。</p>
<p>　　我们可以通过下面命令查看分区细节信息。</p>
<pre><code>./bin/kafka-topics.sh --zookeeper localhost:2181/kafka --describe --topic topic-create 
</code></pre><p>　　来查看信息。</p>
<p>　　我们可以更加详细的分配主题副本不同节点的分配。</p>
<pre><code>./bin/kafka-topics.sh --zookeeper localhost:2181/kafka --create --topic topic-createsss --replica-assignment 2:0,0:1,1:2,2:1
</code></pre><p>　　这很好理解，2个副本，4个分区，分区1的副本是2,0，分区2的副本是0,2之类的。前面说过了，副本必须在不同节点上面，因此如果你设置的节点是一样的，会报错。同时如果设置的分区的副本数不一样也不行，也会报错。比如：2:0,0,2:1这种情况，第二个分区的副本是1个其余分区的副本是2个。2:0,,2:1跳过某个分区也是不行的。<br>　　其次，我在集群配置说了，我们可以在主题里面设置参数从而覆盖broker参数。</p>
<pre><code>./bin/kafka-topics.sh --zookeeper localhost:2181/kafka --create --topic topic-dh --partitions 4 --replication-factor 1 --config cleanup.policy=compact --config max.message.bytes=10000
</code></pre><p>　　同时我们创建主题时不能同名。可以加这么一个参数来限定。</p>
<pre><code>./bin/kafka-topics.sh --zookeeper localhost:2181/kafka --create --topic topic-dh --partitions 4 --replication-factor 1　--if-not-exists
</code></pre><p>　　kafka内部做埋点处理时会根据主题的名称来命名metrics的名称，且将”.”改成”_”,因此topic.1_2和topic_1_2的metrics的名称都是topic_1_2，因此注意是否重名。<br>　　broke支持指定机架信息，如果制定了机架信息，则在分区副本分配是会尽可能的让分区副本分配到不同机架上，通过参数broke.rack=RACK1来配置的。如果集群里面部分broke指定了机架，部分没有指定，那么依旧会报错。</p>
<p>　　使用这个脚本创建主题，也就是这个kafka-topic.sh,本质上是调用kafka.admin.TopicCommand这个类实现对主题的管理，我们可以直接使用这个类来创建主题。demo如下：</p>
<pre><code>String[] opts = new String[] { &quot;--zookeeper&quot;, &quot;:/2181/kafka&quot;, &quot;--create&quot;, &quot;--replication-factor&quot;, &quot;1&quot;,
            &quot;--topic&quot;, &quot;topic-create-api&quot; };
    kafka.admin.TopicCommand.main(opts);
</code></pre><h2 id="分区副本的分配"><a href="#分区副本的分配" class="headerlink" title="分区副本的分配"></a>分区副本的分配</h2><p>　　除了前面说的–replica-assignment参数来直接指定副本分配情况，如果没有的话，则是按照内部逻辑进行处理的，有２个方案。有前面我说的机架信息，和没有机架信息。<br>　　没有机架信息的方案如下：（ps 这是scala代码）</p>
<pre><code>val rand = new Random
private def assignReplicasToBrokersRackUnaware(
nPartitions: Int, //分区数
replicationFactor: Int, //副本因子
brokerList: Seq[Int], //集群中broker列表
fixedStartIndex: Int, //起始索引。默认为-1
startPartitionId: Int //起始分区编号，默认-1
): Map[Int, Seq[Int]] //返回值类型
= {
val ret = mutable.Map[Int, Seq[Int]]() //声明一个map
val brokerArray = brokerList.toArray //获取brokerid的列表
//起始索引小于0,那么从brokerid列表里面获取一个随机的有效值
val startIndex = if (fixedStartIndex &gt;= 0) fixedStartIndex else rand.nextInt(brokerArray.length)
//确保起始分区大于0
var currentPartitionId = math.max(0, startPartitionId)
//指定副本间隔
var nextReplicaShift = if (fixedStartIndex &gt;= 0) fixedStartIndex else rand.nextInt(brokerArray.length)
//这是一个for循环，scala语法看着好难受，遍历所有分区。
for (_ &lt;- 0 until nPartitions) {
  if (currentPartitionId &gt; 0 &amp;&amp; (currentPartitionId % brokerArray.length == 0))
    nextReplicaShift += 1
  //获取第一个副本索引
  val firstReplicaIndex = (currentPartitionId + startIndex) % brokerArray.length
  //生成一个该分区副本集合
  val replicaBuffer = mutable.ArrayBuffer(brokerArray(firstReplicaIndex))
  //保存该分区的所有副本分配的broker集合
  for (j &lt;- 0 until replicationFactor - 1)
    //为其余副本分配broker
    replicaBuffer += brokerArray(replicaIndex(firstReplicaIndex, nextReplicaShift, j, brokerArray.length))
  //保存该分区的副本分配信息
  ret.put(currentPartitionId, replicaBuffer)
  //继续下一个分区
  currentPartitionId += 1
}
ret
}
private def replicaIndex(firstReplicaIndex: Int, secondReplicaShift: Int, replicaIndex: Int, nBrokers: Int): Int = {
val shift = 1 + (secondReplicaShift + replicaIndex) % (nBrokers - 1)
(firstReplicaIndex + shift) % nBrokers
}
</code></pre><p>　　这个算法使得分区副本分配的很均匀。差不多正好。<br>　　指定机架信息和没指定机架信息本质上差不多。一个机架可以分配多个broker节点，但是满足下面条件的broker不可哟添加到当前分区的副本列表里面。<br>　　１.此broker所在机架已经有一个broker在这个分区的副本列表里面，且其他机架中没有任何的broken在该分区的副本列表里面。</p>
<p>　　２.此broker已经在在列表，且其他broker不在。创建主题时实质上是在zookeeper中的/kafka/brokers/topics节点下创建和该主题对应的子节点，并且写入副本分配信息，且在/config/topics节点下创建该节点对应的子节点并且主题配置信息。kafka创建主题的实质上动作是交给控制器异步完成的。<br>　　因此我们可以直接通过创建规则下的节点，来直接创建一个新的主题。这样我们可以绕过一些规则，比如我们创建主题分区的时候都是从０开始计数。我们通过创建zookeeper节点就不用从0开始累加了。</p>
<h2 id="查看主题"><a href="#查看主题" class="headerlink" title="查看主题"></a>查看主题</h2><p>　　kafka-topics.sh这个命令有５个指令类型：create,list,describe,alter和delete。其中list和describe是查看主题信息的。</p>
<pre><code>./bin/kafka-topics.sh --zookeeper localhost:2181/kafka -list
</code></pre><p>　　这个命令是查看当前kafka当前所有可用主题。</p>
<pre><code>./bin/kafka-topics.sh --zookeeper localhost:2181/kafka --describe --topic topic-create 
</code></pre><p>　　这个是查看topic-create这个主题的详细信息，可以接多个主题，一次查看多个主题信息。如果没有–topic这个参数则是查看所有主题信息。</p>
<pre><code>./bin/kafka-topics.sh --zookeeper localhost:2181/kafka --describe  --topics-with-overrides
</code></pre><p>　　–topics-with-overrides加这个参数则是查看所有使用了覆盖配置的主题。</p>
<pre><code>./bin/kafka-topics.sh --zookeeper localhost:2181/kafka --describe --topic topic-create --under-replicated-partitions
</code></pre><p>　　这个参数是查询当前主题所有包含失效副本的分区</p>
<pre><code>./bin/kafka-topics.sh --zookeeper localhost:2181/kafka --describe --topic topic-create -unavaliable-partitions 
</code></pre><p> 　　这个参数查看主题中没有lead副本的分区。</p>
<h2 id="修改主题"><a href="#修改主题" class="headerlink" title="修改主题"></a>修改主题</h2><p>　　当一个主题被创建之后，依然允许我们对其做一定修改，比如修改分区个数，修改配置，通过alter指令来完成的。</p>
<pre><code>./bin/kafka-topics.sh --zookeeper localhost:2181/kafka --alter --topic topic-create --partitions 3
</code></pre><p>　　这个是将topic-create的分区修改为3.如此的话可能会有影响。<br>　　目前是不支持分区从多变少的。</p>
<pre><code>./bin/kafka-topics.sh --zookeeper localhost:2181/kafka --alter --topic topic-create   --config max.message.bytes=10000
</code></pre><p>　　这个命令是修改这个主题的配置。</p>
<pre><code>./bin/kafka-topics.sh --zookeeper localhost:2181/kafka --alter --topic topic-create   --delete-config max.message.bytes
</code></pre><p>　　这个是删除之前的配置。使其恢复默认配置。</p>
<p>　　我们一般通过kafka-configs.sh脚本来执行修改主题配置信息。</p>
<h2 id="配置管理"><a href="#配置管理" class="headerlink" title="配置管理"></a>配置管理</h2><p>　　有个脚本kafka-configs.sh是专门对配置进行操作的。可以在运行时修改原有的配置。相对于之前的脚本，主要是可以修改broker,client,users这些配置的配置</p>
<pre><code>./bin/kafka-configs.sh --zookeeper localhost:2181/kafka --describe --entity-type topics --entity-name topic-create
</code></pre><p>　　这个脚本支持查询主题，broker,client,users这些配置的配置，根据–entity-type来区分。–entity-name 显然指的是类型名字。这个查出来的仅仅是配置信息。和之前脚本不一样，这个命令本质上是从zookeeper上读取相关节点信息，/config/type/name</p>
<pre><code>./bin/kafka-configs.sh --zookeeper localhost:2181/kafka --alter --entity-type topics --entity-name topic-create --add-config cleanup.policy=compact,max.message.bytes=10000
</code></pre><p>　　修改主题使用–add-config来增，改，覆盖之前的配置。</p>
<pre><code>./bin/kafka-configs.sh --zookeeper localhost:2181/kafka --alter --entity-type topics --entity-name topic-create --delete-config cleanup.policy
</code></pre><p>　　删除原配置–delete-config，删除之前被覆盖的配置，恢复为默认配置。</p>
<p>　　使用kafka-configs.sh来修改脚本时，会在对应zookeeper中创建一个节点，并且将变更的配置写入到这个节点。</p>
<h2 id="删除主题"><a href="#删除主题" class="headerlink" title="删除主题"></a>删除主题</h2><p>　　当某个主题已确定不在使用时，为了节约资源，我们最好删除。</p>
<pre><code>./bin/kafka-topics.sh --zookeeper localhost:2181/kafka --delete --topic topic-create   
</code></pre><p>　　这个参数和delete.topic.enable有关，默认为true,如果为false,那么删除操作就会被忽略。同时我们删除内部主题，不存在的主题时都会报错。当然加了–if-exists这个命令后就会忽略报错。<br>　　使用这个命令的本质是在zookeeper上的/admin/delete-topics路径下创建一个和待删除主题同名的节点，和创建主题一样，真正的删除动作是kafka的控制器完成的。 </p>
<p>　　因此我们可以通过创建一个这样的节点来删除主题。</p>
<pre><code>create /admin/delete_topics/topic_delete &quot;&quot;
</code></pre><p>　　如此就删除了这个叫做topic_delete的主题，同理，我们也可以创建按照规则的主题。</p>
<p>　　更加手动的方式，一个主题其信息元数据存在zookeeper的 /brokers/topics和config/topics路径下的，消息数据则是存在log.dir我们配置的路径下面，我们只需要删除这些东西即可，规则如下：先删除brokers/topics和config/topics路径下的节点，2者顺序任意，然后删除其数据文件。</p>
<h1 id="KafkaAdminClient"><a href="#KafkaAdminClient" class="headerlink" title="KafkaAdminClient"></a>KafkaAdminClient</h1><pre><code>package com.dh.kafka;

import java.util.Collections;
import java.util.Properties;
import java.util.concurrent.ExecutionException;
import org.apache.kafka.clients.admin.AdminClient;
import org.apache.kafka.clients.admin.AdminClientConfig;
import org.apache.kafka.clients.admin.CreateTopicsResult;
import org.apache.kafka.clients.admin.NewTopic;

/**
 * 创建主题试试 使用KafkaAdminClient来创建主题。
* 
 * @author Lenovo
 *
 */
public class CreateTopic {
public static void main(String[] args) {
    String brokerList = &quot;192.168.147.132:9092&quot;;
    String topic = &quot;topic-admin&quot;;
    Properties props = new Properties();
    props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, brokerList);
    props.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG, 30000);
    AdminClient client = AdminClient.create(props);
    NewTopic newTopic = new NewTopic(topic, 4, (short) 1);
    CreateTopicsResult result = client.createTopics(Collections.singleton(newTopic));
    try {
        result.all().get();
    } catch (InterruptedException | ExecutionException e) {
        // TODO Auto-generated catch block
        e.printStackTrace();
    }
    client.close();
}
}
</code></pre><p>　　创建主题，给出了案例，很简单，创建主题时，有很多构造方法。我们先看其属性</p>
<pre><code>public class NewTopic {
private final String name;
private final int numPartitions;
private final short replicationFactor;
private final Map&lt;Integer, List&lt;Integer&gt;&gt; replicasAssignments;
private Map&lt;String, String&gt; configs = null;
｝
</code></pre><p>　　replicasAssignments这个参数是分区编号－broke列表。可以手动指定分区和broke的分配。configs则是配置的设定，我们可以给主题设置config.从而覆盖broke的配置。</p>
<p>　　AdminClient使用自己内置的协议来管理发送请求等功能。自己使用相关协议发送，然后再用相关协议解析。</p>
<h2 id="主题的合法性"><a href="#主题的合法性" class="headerlink" title="主题的合法性"></a>主题的合法性</h2><p>　　我们一般禁止客户端直接创建主题，不利于运维维护。但是前面AdminClient却可以直接创建。kafka有一个参数，叫做creat.topic.policy.class.name默认为null.提供了一个入口用来验证主题创建的合法性。我们自定义一个实现CreateTopicPolicy接口的类，然后让上面的参数指向我们这个类的全限定名。在启动服务，即可。这个类要在服务端，打个jar扔到classpath里面</p>
<h2 id="优先副本选举"><a href="#优先副本选举" class="headerlink" title="优先副本选举"></a>优先副本选举</h2><p>　　我们创建一个分区为３，副本为３的主题，必须要大于３的broke来支持。然后重启其中一个broke，那么lead副本可能不均衡了。由于消费者都是直接从lead副本交互数据，所以影响蛮大的。而创建和修改主题时，会有一个叫做优先副本的概念，kafka会通过一定的方式促使优先副本的选举为lead副本，从而使得分区平衡。当然不同broke的负载是不一样的，有的高，有的低。<br>　　这个方式是在broke端配置的，将auto.leader.rebalance.enable设置为true（默认也是true）,开启后，kafka的控制器会启动一个定时任务来轮训所有broke节点，计算一个值（非优先副本leader副本/分区总数）超过leader.imbalance.per.broker.percentage默认是0.1，超过这个值就会开启优先副本选举以来分区平衡，定时器的周期是leader.imbalance.check.interval.seconds控制，默认300秒。<br>　　但是生产环境是不建议开启的，选举优先节点时会阻塞业务，不好，而且分区平衡也不是负载均衡。我们可以在一个时间内，手动去执行分区平衡。是通过执行这个脚本命令。</p>
<pre><code>./bin/kafka-preferred-replica-election.sh --zookeeper localhost:2181/kafka 
</code></pre><p>　　这个命令是扫描集群里面所有分区，如果分区过多，可能执行失败，因为在选举过程，具体的元数据信息会被存入到zookeeper的/admin/preferred-replica-election节点，如果这些数据超过了zk默认节点大小（默认1M）因此我们可以path-to-json-file参数来小批量的对部分分区执行优先副本选举，通过path-to-json-file来指定一个json文件。</p>
<h2 id="分区重分配"><a href="#分区重分配" class="headerlink" title="分区重分配"></a>分区重分配</h2><p>　　前面我们创建主题时，下线其中一个broke，这个节点的副本都会变得不可用，如果不修复，这个分区负载会一直这样，新增一个节点，也是如此。我们可以用kafka-reassign-partitions.sh来执行分区重分配的任务。原理就是复制，然后删除。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://skydh.github.io/2019/06/25/kafka基础-消费者/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="skydh">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="learning, progress, future.">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/25/kafka基础-消费者/" itemprop="url">kafka入门003 -消费者</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-25T16:24:53+08:00">
                2019-06-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="消费者与消费组"><a href="#消费者与消费组" class="headerlink" title="消费者与消费组"></a>消费者与消费组</h2><p>　　每个消费者都有一个对应的消费组，当消息发送到主题后。只会被投递给订阅他的每个消费组中的某一个消费者。</p>
<p>　　按照kafka默认原则。主题X有a,b,c,d。4个分区，有2个消费者组A（C0，C1,C2,C3）4个消费者，B(C4,C5)2个消费者都订阅了这个主题，那么消费者A组中每个消费者都分配一个分区，消费者B每个消费者分配2个分区。消费者只能消费自己所分配分区的消息。换句话说，消费者组之间是共享的，消费组中的消费者是互斥的。</p>
<p>　　同时我们可以动态的增加消费者增加消费能力，比如B消费者组，我给他再加2个消费者，那么这个B消费者组就会的4个消费者就会被一个分区一个消费者，如果再加一个消费者没什么卵用，因为分区数目&lt;消费者数目，没有多余的分区可以分配给消费者了。同时，假设组内某个实例挂掉了，Kafka能够自动检测到，然后把这个Failed实例之前负责的分区转移给其他活着的消费者。这个过程就是Kafka中大名鼎鼎的“重平衡”（Rebalance）。</p>
<p>　　消息中间件有２种消息投递方式：即点对点模型（Peer to Peer，P2P）和发布订阅模型。这里面的点对点指的是同一条消息只能被下游的一个消费者消费，其他消费者则不能染指。而发布订阅则是将消息发送到某个主题，消息订阅者都可以从这个主题里面获取消息。<br>　　对于kafka来说。所有的消费者都是一个消费组的时候就是点对点模型，消费者隶属于不同消费者组时，则是发布订阅模型。</p>
<h2 id="消息格式"><a href="#消息格式" class="headerlink" title="消息格式"></a>消息格式</h2><pre><code>public class ConsumerRecord&lt;K, V&gt; {
    public static final long NO_TIMESTAMP = RecordBatch.NO_TIMESTAMP;
    public static final int NULL_SIZE = -1;
    public static final int NULL_CHECKSUM = -1;

    private final String topic;
    private final int partition;
    private final long offset;
    private final long timestamp;
    private final TimestampType timestampType;
    private final int serializedKeySize;
    private final int serializedValueSize;
    private final Headers headers;
    private final K key;
    private final V value;
    private volatile Long checksum;
}
</code></pre><p>　　简单的我就不说了和生产者差不多，offset表示消息所属分区的偏移量。</p>
<h2 id="参数配置"><a href="#参数配置" class="headerlink" title="参数配置"></a>参数配置</h2><p>　　bootstrap.servers：消费者客户端连接kafka集群所需的broker地址清单，一般写2个，host:port,host1:port1,不要全部写出来，因为生产者会从给定的broke里面查出其他broker的信息，写2个是为了防止一个宕机了，生产者依旧可以连接到kafak。</p>
<p>　　key.deserializer，value.deserializer：和生产者的key.serializer，value.serializer相对应，消费者从broker服务器接受的消息是字节数组存在的，这个就是指定，key和value的序列化操作的反序列化器，来进行数据的反序列化。</p>
<p>　　group.id:消费者隶属的消费组的名称。</p>
<p>　　client.id:对应kafka客户端的id，也就是客户端的名字。</p>
<p>　　fetch.min.bytes:表示poll能从kafka拉取的最小数据量，默认1b</p>
<p>　　fetch.max.bytes：最大，是50m,但是不是绝对的。真正最大值是message.max.bytes来配置</p>
<p>　　fetch.max.wait.ms:默认500ms,在这个时间内，如果没有fetch.min.bytes的限定值，还是返回。</p>
<p>　　max.partition.fetch.bytes:每个分区返回给consumer最大的数据量，默认1m。</p>
<p>　　max.poll.records:一次poll最大的消息数。默认500.</p>
<p>　　connections.max.ide.ms:多久之后干不限制的连接，默认9分钟。</p>
<p>　　exclude.internal.topic:默认true，没法通过正则来订阅内部主题。</p>
<p>　　receive.buffer.bytes:设定socket的接受缓冲区大小，默认大小：64kb.</p>
<p>　　send.buffer.bytes:设定sockect发送缓冲区大小，默认是128kb.</p>
<p>　　request.timeout.ms:配置consumer等待请求响应时间。默认是30000ms。<br>　　<br>　　metadata.max.age.ms:配置元数据更新时间，默认5分钟，即便元数据没有任何变化。</p>
<p>　　reconnect.backoff.ms：配置尝试重新连接指定主机之前的等待时间。避免某些故障的情况下，频繁发送。默认50ms</p>
<p>　　retry.backoff.ms:配置尝试重新发送失败到指定主题分区的等待时间，避免某些故障的情况下，频繁发送。默认100ms<br>　　１</p>
<p>　　isolation.level:配置消费者的事务隔离级别。默认是read_uncommitted,可以消费到HW.
　　
　　</p>
<h2 id="代码拼写"><a href="#代码拼写" class="headerlink" title="代码拼写"></a>代码拼写</h2><pre><code>props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
</code></pre><p>　　我们写这些参数时可能某个字符写错了。因此可以用ConsumerConfig来替代。
　　</p>
<h2 id="订阅主题和分区"><a href="#订阅主题和分区" class="headerlink" title="订阅主题和分区"></a>订阅主题和分区</h2><p>　　通过KafkaConsumer的subscribe方法来订阅主题。可以订阅多个主题。因为接收对象是个集合。下面是常见的一个接口方法。</p>
<pre><code>public void subscribe(Collection&lt;String&gt; topics) {
    subscribe(topics, new NoOpConsumerRebalanceListener());
}
</code></pre><p>　　还有一个接口是通过正则表达式来订阅的，只要是符合要求的，都会被订阅，即便是后面新增的也是如此。</p>
<pre><code>@Override
public void subscribe(Pattern pattern) {
    subscribe(pattern, new NoOpConsumerRebalanceListener());
}
</code></pre><p>　　比如</p>
<pre><code>consumer.subscribe(Pattern.compile(&quot;topic-.*&quot;));
</code></pre><p>　　这个就是以topic开头的主题都会被订阅。<br>　　在kafka客户端里面TopicPartition类表示分区。</p>
<pre><code>public final class TopicPartition implements Serializable {

private int hash = 0;
private final int partition;
private final String topic;
}
</code></pre><p>　　主要２个属性，主题名称，和分区编号。<br>　　<br>    consumer.assign(Collections.singletonList(new TopicPartition(topic,0)));<br>　　我们可以通过assign方法订阅主题里面的某个分区。<br>　　但是主题里面有哪些分区时元数据信息。我们可以获取到这些信息，通过生产者的partitionsFor获取分区元数据信息。</p>
<pre><code>List&lt;PartitionInfo&gt; list=consumer.partitionsFor(topic);

public class PartitionInfo {
    private final String topic;
    private final int partition;
    private final Node leader;
    private final Node[] replicas;
    private final Node[] inSyncReplicas;
    private final Node[] offlineReplicas;
}
</code></pre><p>　　topic是主题，partition分区号，leader分区的lead副本，replicas分区的AR集合，inSyncReplicas是ISR集合，offlineReplicas是OSR集合。<br>　　我们可以先通过partitionsFor方法获取到元数据信息，在进行分区订阅。</p>
<pre><code>consumer.unsubscribe();
</code></pre><p>　　我们可以通过上述方法取消订阅。　<br>　　我们订阅主题消息的方式有３个，分别代表不一样的状态，他们是互斥的。</p>
<h2 id="反序列化"><a href="#反序列化" class="headerlink" title="反序列化"></a>反序列化</h2><p>　　和生产者一文的大致类似，只是反着来了，没啥好说的。</p>
<h2 id="消息消费"><a href="#消息消费" class="headerlink" title="消息消费"></a>消息消费</h2><p>　　消息的消费有２个模式，推：服务器主动把消息推给消费者，拉：消费者主动向服务器发起请求拉取消息。<br>　　kafka则是通过poll方法去拉取的。<br>　　在我们代码里面，我们就是循环不断调用poll来拉取数据的。poll方法有个参数是时间，表示消费者缓冲区里面没有数据时阻塞。<br>　　我们可以根据消息分区维度进行消费。</p>
<pre><code>ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(1000));
            for (TopicPartition tp : records.partitions()) {
                for (ConsumerRecord&lt;String, String&gt; record : records.records(tp))
                    System.out.printf(record.value());
            }
</code></pre><p>　　同理，也可以根据消息主题进行区分消费。主要用到了下面的方法：</p>
<pre><code>  public Iterable&lt;ConsumerRecord&lt;K, V&gt;&gt; records(String topic) {
    if (topic == null)
        throw new IllegalArgumentException(&quot;Topic must be non-null.&quot;);
    List&lt;List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; recs = new ArrayList&lt;&gt;();
    for (Map.Entry&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; entry : records.entrySet()) {
        if (entry.getKey().topic().equals(topic))
            recs.add(entry.getValue());
    }
    return new ConcatenatedIterable&lt;&gt;(recs);
}
</code></pre><h2 id="位移提交"><a href="#位移提交" class="headerlink" title="位移提交"></a>位移提交</h2><p>　　在kafka服务器端每个消息有个offset用来表示消息所在分区的位置，我们用偏移量来表示，而消费者端也有一个offset，表示消费者消费到分区的哪个消息了，我们用位移来表示。<br>　　消费者消费的消息都是没有消费的，因此这个位移必须保存且持久化，这个位移保存在kafka内部的主题_consumer_offsets中，每个消费组一个消费位移。消费者在消费完消息后，需要再这里提交消费位移的提交。记住消费者要提交的消费位移不是已经消费的消息位置的最后一个，而是已消费消息位置+1，这个没有消费的位置。而committed offset则表示已经提交过的消费位移。</p>
<pre><code>@Override
public long position(TopicPartition partition) {
    return position(partition, Duration.ofMillis(defaultApiTimeoutMs));
}
</code></pre><p>　</p>
<pre><code>@Override
public OffsetAndMetadata committed(TopicPartition partition) {
    return committed(partition, Duration.ofMillis(defaultApiTimeoutMs));
}
</code></pre><p>　　消费者的２个接口可以分别获取到上述的２个值。这２个值在某些情况是一样的，但是在某些情况是不一样的。</p>
<p>　　对于位移提交是要把握时机的，比如你拉取了一大堆数据，还没有消费完就出了bug,你却已经提交了消费位移，那么就会造成消息丢失。如果全部不提交，那也可能造成消息重复。<br>　　kafka消息位移提交的策略是自动提交，定期提交，每隔5秒。是以下参数配置<br>　　enable.auto.commit默认是true,<br>　　auto.commit.interval.ms默认是5s。</p>
<p>　　在默认情况下，刚提交完一次位移，然后拉取一批消息消费，在下一次自动提交消费位移钱，消费者宕机了，那么重启后，或者转给别的消费者后，就要重复消费。<br>　　而消息丢失呢，主要是你代码造成的，比如a线程拉取数据到本地缓存，b线程处理缓存的数据，某个点，提交了位移，因为数据确实消费完了，但是b线程没有消费到缓存中对应的那个点就宕机了，那么就造成了数据丢失。</p>
<p>　　当然上述情况都是异常情况导致的。正常情况不会如此，因此kafka提供了手动提交位移的操作。<br>　　手动提交可以是同步或者异步的。</p>
<pre><code>consumer.commitAsync();
consumer.commitSync();
</code></pre><p>　　对于同步提交依然可能造成消息重复。这个还是批量提交。因为可能中间有异常，但是我们可以把异常捕捉，在针对性的处理。</p>
<p>　　更加精细的提交方式</p>
<pre><code>@Override
public void commitSync(final Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets) {
    commitSync(offsets, Duration.ofMillis(defaultApiTimeoutMs));
}
</code></pre><p>　　用这个接口的方法，可以实现每消费一个消息就提交一次位移。一般来说不会这么做，太消耗性能了。一般来说，我们大多是根据分区的粒度来划分提交位移的界限，代码如下：看了你就懂了。</p>
<pre><code>ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(1000));
            for (TopicPartition tp : records.partitions()) {
                List&lt;ConsumerRecord&lt;String, String&gt;&gt; pRecords = records.records(tp);
                for (ConsumerRecord&lt;String, String&gt; record : pRecords)
                    System.out.printf(record.value());
                long lastOffset = pRecords.get(pRecords.size() - 1).offset();
                consumer.commitSync(Collections.singletonMap(tp, new OffsetAndMetadata(lastOffset + 1)));
            }
</code></pre><p>　　异步提交则是不一样的，执行时，消费者线程不会被阻塞，可能提交消费位移的结果还没有返回之前就开始了新一轮的拉取操作。他有个回调方法很不错。</p>
<pre><code>consumer.commitAsync(new OffsetCommitCallback() {
                    @Override
                    public void onComplete(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, Exception exception) {
                        //这里操作

                    }
                });
</code></pre><p>　　由于是异步，可能出现下面问题consumer.commitAsync失败了，你直接重试，然后下面一个成功了，你重试也成功了，然后宕机重启，完了，消息又重复消费了。一般情况，不需要重试，很少发生，后面的会补上。同时我们可以在异步提交之后，在try一下，为了保证消息位移的提交在加个同步提交。</p>
<h2 id="控制消费"><a href="#控制消费" class="headerlink" title="控制消费"></a>控制消费</h2><p>　　有时我们需要暂停某些分区的消费，当达到某些时间时再恢复。</p>
<pre><code>consumer.pause(partitions);
consumer.resume(partitions);
</code></pre><p>　　这个是暂停和恢复２个方法。</p>
<pre><code>Set&lt;TopicPartition&gt; set=consumer.paused();
</code></pre><p>　　这个是查看被暂停分区的方法。</p>
<p>　　除了while (isRunning.get()) {}这个方法来设定是否关闭连接外，还可以同期线程外的wakeup方法来退出poll的逻辑。他会抛出一个可捕捉的异常。我们要处理。但是要注意同时关闭资源，调用close方法。</p>
<h2 id="指定位移消费"><a href="#指定位移消费" class="headerlink" title="指定位移消费"></a>指定位移消费</h2><p>　　这些情况下，可能找不到消息位移。<br>　　１.新建立的消费组。<br>　　２．消费组新的一个消费者订阅了一个新的主题。<br>　　３．_consumer_offsets这个主题中有关这个消费组的位移信息因为过期被删除。<br>　　当kafka找不到消息消费位移后，就会根据消费者客户端参数auto.offset.reset来读取，这个配置默认读取latest,表示从分区末尾开始读取消息。如果改成earlist则是从起始处读取配置。如果改成none，则是抛出异常。只能是这3个值。</p>
<p>　　除了找不到消费位移，位移越界也会触发auto.offset.reset操作。<br>　　我们使用poll  来批量抓取消息时是黑盒的，但是我们可以通过seek细致的抓取消息处理。</p>
<pre><code>public void seek(TopicPartition partition, long offset)
</code></pre><p>　　他可以为每个分区设定位移。</p>
<pre><code>Set&lt;TopicPartition&gt; set=consumer.assignment();
</code></pre><p>　　<br>　　获取当前消费者被分配的分区。</p>
<p>　　我们先poll一次，获取到元数据等信息，然后对分区设置位移，开始消费（ps，不要担心poll会造成消息丢失，因为我们要设定自己的新位移，那次poll的消息是无意义的）</p>
<p>　　需求来了，我们需要消费昨天８点之后的消息。</p>
<pre><code>@Override
public Map&lt;TopicPartition, OffsetAndTimestamp&gt; offsetsForTimes(Map&lt;TopicPartition, Long&gt; timestampsToSearch) {
    return offsetsForTimes(timestampsToSearch, Duration.ofMillis(defaultApiTimeoutMs));
}
</code></pre><p>　　参数是map,key是分区，value是时间戳。这个方法会返回时间戳大于等于待查询时间的第一条消息的位置和时间戳。我们可以利用这个位置来针对性的消费。</p>
<p>　　位移越界也会造成auto.offset.reset启用。</p>
<p>　　我们同时可以把消费位移保存到db里面，然后下次消费时从db读取位移，再消费。</p>
<h2 id="再均衡"><a href="#再均衡" class="headerlink" title="再均衡"></a>再均衡</h2><p>　　前面说了，基于主题的可以再均衡，直接订阅分区的无法进行再均衡。就是更前面我们说的，一个消费组，新增或者减少消费者，会进行均衡处理。一个分区被重新分配给另一个消费者时，消费者当前的状态会丢失。同时　再进行再均衡期间，消息是无法被读取的。如果某个消费者消费了消息，消息位移还没有提交，那么这个分区被分配给新的消费者时，那么就会造成消息重复消费。<br>　　这个再均衡和我们之前说的NoOpConsumerRebalanceListener相关。这个监听器是用来设定发生再均衡前后的一些准备和收尾工作。主要下面２个方法处理</p>
<pre><code>@Override
public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; partitions) {}
</code></pre><p>　　这个方法是在均衡之前，消费者停止读取消息之后被调用。可以用这个方法来提交消息位移。</p>
<pre><code>@Override
public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions) {}
</code></pre><p>　　这个方法在重新分配分区之后和消费者重新开始读取之前调用。</p>
<pre><code>Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets=new HashMap&lt;&gt;();
    consumer.subscribe(Collections.singletonList(topic), new ConsumerRebalanceListener() {
        @Override
        public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions) {
            consumer.commitSync(offsets);
            offsets.clear();
        }

        @Override
        public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; partitions) {

        }
    });
</code></pre><p>　　我们需要把消费位移记录到map里面。每一次消费消息时，把这个消息的分区和分区位移加到这个map里面。</p>
<h2 id="消费者拦截器"><a href="#消费者拦截器" class="headerlink" title="消费者拦截器"></a>消费者拦截器</h2><pre><code>public interface ConsumerInterceptor&lt;K, V&gt; extends Configurable {
    public ConsumerRecords&lt;K, V&gt; onConsume(ConsumerRecords&lt;K, V&gt; records);
    public void onCommit(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets);
    public void close();
}
</code></pre><p>　　这个拦截器，必须实现这个接口，主要２个方法，onConsume方法是poll方法返回之前调用的，onCommit则是提交完消费位移后调用的。<br>　　注意要在配置里面添加：</p>
<pre><code>props.put(ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, value)
</code></pre><p>　　和生产者一样，可以写成链式。</p>
<h2 id="多线程实现"><a href="#多线程实现" class="headerlink" title="多线程实现"></a>多线程实现</h2><p>　　生产者是线程安全的，但是消费者不是线程安全的。</p>
<pre><code> private void acquire() {
    long threadId = Thread.currentThread().getId();
    if (threadId != currentThread.get() &amp;&amp; !currentThread.compareAndSet(NO_CURRENT_THREAD, threadId))
        throw new ConcurrentModificationException(&quot;KafkaConsumer is not safe for multi-threaded access&quot;);
    refcount.incrementAndGet();
}
</code></pre><p>　　他有个方法是专门检测当前是否只有一个线程操作。</p>
<pre><code>  private void release() {
    if (refcount.decrementAndGet() == 0)
        currentThread.set(NO_CURRENT_THREAD);
}
</code></pre><p>　　和这个方法相互配合，类似加锁和解锁。注意compareAndSet(a,b)这个方法是a先和这个类的数据比较，如果一样则把b放进去替代数据。<br>　　为了提高消费者的消费能力，防止broke里面堆积的消息过多。从而导致消息还没被消费就丢失。下面几个策略可以提高消费能力。<br>　　１.每个线程实现一个KafkaConsumer，但是线程数目不能超过分区数，前面解释过。<br>　　２．去除再平衡效果，直接以分区作为基本单位进行消费。多个线程消费一个分区。一般不会推荐这么做的。<br>　　直接启动多个消费者线程，有个缺点就是，每个线程都要维护一个tcp连接。这会造成很大的系统开销。<br>　　<br>　　３.对于kafka来说poll的速度是很快的，但是一般是处理消息是很消耗性能。如果改动了这一块，那么对性能的提升很大。因此我们可以吧消息处理模块变成多线程处理。但是对顺序消费有问题。同时位移提交也要处理。可以考虑共享变量Map<topicpartition, offsetandmetadata=""> offsets=new HashMap&lt;&gt;()同时注意覆盖问题，调用时;还要注意消息丢失问题，比如2个处理业务的线程，a发生异常，没有提交，b在a后面的，然后成功提交，那么a的数据就丢失了。<br>　　可以考虑tcp的滑动窗口模式。把数据丢到一个缓存队列里面。然后多个设定一个固定的大小的窗口，也就是，最小游标和最大游标。然后多线程（业务线程）充窗口拉数据，处理完了就回去报告，接着拉取数据，同时窗口判断自己是否可移动。</topicpartition,></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://skydh.github.io/2019/06/24/kafka集群配置/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="skydh">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="learning, progress, future.">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/24/kafka集群配置/" itemprop="url">kafka入门004 -集群配置</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-24T14:20:55+08:00">
                2019-06-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="配置存储的参数"><a href="#配置存储的参数" class="headerlink" title="配置存储的参数"></a>配置存储的参数</h2><p>　　log.dirs：这是非常重要的参数，指定了Broker需要使用的若干个文件目录路径。这个参数是没有默认值的，必须由你亲自指定。样例：比如/home/kafka1,/home/kafka2,/home/kafka3这样。如果有条件的话你最好保证这些目录挂载到不同的物理磁盘上。多个磁盘可以提高读写性能，实现故障转移。<br>　　log.dir：能表示单个路径，它是补充上一个参数用的。一般不需要设置</p>
<h2 id="zookeeper参数"><a href="#zookeeper参数" class="headerlink" title="zookeeper参数"></a>zookeeper参数</h2><p>　　zookeeper.connect：比如我可以指定它的值为zk1:2181,zk2:2181,zk3:2181。2181是ZooKeeper的默认端口如果多个kafka集群共用一个zookeeper集群时，如何配置处理。在最后的一个zookeeper后加个别名例如：zk1:2181,zk2:2181,zk3:2181/kafka</p>
<h2 id="broker通信相关"><a href="#broker通信相关" class="headerlink" title="broker通信相关"></a>broker通信相关</h2><p>　　listeners这个配置的，前文由于写demo介绍过。</p>
<p>　　listeners：学名叫监听器，其实就是告诉外部连接者要通过什么协议访问指定主机名和端口开放的Kafka服务。<br>　　advertised.listeners：和listeners相比多了个advertised。Advertised的含义表示宣称的、公布的，就是说这组监听器是Broker用于对外发布的。<br>　　host.name/port：列出这两个参数就是想说你把它们忘掉吧，压根不要为它们指定值，毕竟都是过期的参数了。</p>
<h2 id="topic"><a href="#topic" class="headerlink" title="topic"></a>topic</h2><p>　　auto.create.topics.enable：是否允许自动创建Topic。一般为false，让运维管控。免得出现一堆奇葩命名的<br>　　unclean.leader.election.enable：是否允许Unclean Leader选举。每个分区存在多个副本，有的副本落后lead太多数据，但是lead和其他不落后的都挂了，如果为true，那么这个分区可能数据丢失，因为会选举落后很多的副本作为lead<br>　　auto.leader.rebalance.enable：是否允许定期进行Leader选举。它的值为true表示允许Kafka定期地对一些Topic分区进行Leader重选举，这个是强行选举，因此需要设置为false.</p>
<h2 id="数据留存"><a href="#数据留存" class="headerlink" title="数据留存"></a>数据留存</h2><p>　　log.retention.{hour|minutes|ms}：这是个“三兄弟”，都是控制一条消息数据被保存多长时间。从优先级上来说ms设置最高、minutes次之、hour最低。<br>　　log.retention.bytes：这是指定Broker为消息保存的总磁盘容量大小。<br>　　message.max.bytes：控制Broker能够接收的最大消息大小</p>
<h2 id="参数级别"><a href="#参数级别" class="headerlink" title="参数级别"></a>参数级别</h2><p>　　我们可以在broke端加参数，也可以在topic端添加参数。参数意义一样的，topic端会覆盖broke端。不同消息主题有不同的需求，比如A主题需要消息留存时间2h,B主题则仅仅需要5m,那么们broke端只能设置一个最长的参数来确定，但是有了topic主题后，我们可以针对性设置。</p>
<p>　　retention.ms：规定了该Topic消息被保存的时长。默认是7天，即该Topic只保存最近7天的消息。一旦设置了这个值，它会覆盖掉Broker端的全局参数值。</p>
<p>　　retention.bytes：规定了要为该Topic预留多大的磁盘空间。和全局参数作用相似，这个值通常在多租户的Kafka集群中会有用武之地。当前默认值是-1，表示可以无限使用磁盘空间。</p>
<p>　　我们可以在用户创建kafka的topic，和修改topic时可以增加修改topic参数。</p>
<h2 id="jvm参数"><a href="#jvm参数" class="headerlink" title="jvm参数"></a>jvm参数</h2><p>　　一般最好是java1.8,堆大小最好6GB。我们必须在启动kafka之前加好这些环境变量。<br>　　KAFKA_HEAP_OPTS：指定堆大小。KAFKA_JVM_PERFORMANCE_OPTS：指定GC参数。<br>　　然后正常启动即可。</p>
<h2 id="操作系统参数"><a href="#操作系统参数" class="headerlink" title="操作系统参数"></a>操作系统参数</h2><p>　　文件描述符，可以将其设置的大一点。<br>　　文件系统类型，XFS性能更高。<br>　　swap,内存交换空间，这个值设置的尽量小一点，但是不要设置为0。因为那样内存不够了，会直接杀死任意一个进程。<br>　　flush落盘时间，这个值可以大一点。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://skydh.github.io/2019/06/20/kafka基础-生产者/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="skydh">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="learning, progress, future.">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/20/kafka基础-生产者/" itemprop="url">kafka入门002 -生产者</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-20T10:30:55+08:00">
                2019-06-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="消息格式"><a href="#消息格式" class="headerlink" title="消息格式"></a>消息格式</h2><pre><code>public class ProducerRecord&lt;K, V&gt; {
    private final String topic;
    private final Integer partition;
    private final Headers headers;
    private final K key;
    private final V value;
    private final Long timestamp;
}
</code></pre><p>　　topic是消息主题，partition是分区号，headers一般没用，key是消息的键，同一个key的消息会被发到一个分区里面。</p>
<h2 id="参数配置"><a href="#参数配置" class="headerlink" title="参数配置"></a>参数配置</h2><p>　　bootstrap.servers：生成者客户端连接kafka集群所需的broker地址清单，一般写2个，host:port,host1:port1,不要全部写出来，因为生产者会从给定的broke里面查出其他broker的信息，写2个是为了防止一个宕机了，生产者依旧可以连接到kafak。</p>
<p>　　key.serializer，value.serializer：broker服务器接受的消息是字节数组存在的，这个就是指定，key和value的序列化操作的序列化器。</p>
<p>　　client.id:对应kafka客户端的id，也就是客户端的名字，如果不设置则是默认取名，produce-i</p>
<p>　　retries：对于可重试异常，可重试次数配置，默认是０，ps(KafkaProducer一般有2种异常，一个是可重试异常，比如NetworkException,表示网络异常，可以通过重试解决，一个是不可重试异常，比如RecordToolargeException,表示消息太大，不会重试。)，重试次数超过了配置的值。也会抛出异常。<br>　　retry.backoff.ms：这个主要是和上面的retries相对应的。是重试时间间隔。默认是100</p>
<p>　　props.put(ProducerConfig.ACKS_CONFIG, “0”);<br>　　当为0时，发到服务端就不管了。<br>　　当为1时，只要leader副本写入，就返回，但是可能leader奔溃了，但是其他副本没有拉取到消息。造成数据丢失<br>　　当为-1或者all时，必须所有副本都要同步完才返回。</p>
<p>　　max.request.size:默认值是1m,主要要和broke端配置的message.max.bytes相对应。</p>
<p>　　max.in.flight.requests.per.connection为1时，可以保证顺序消费。<br>　　compression.type:默认为none，可以配置为gzip,lz4等压缩方式，可以减小网络io但是会造成时延。</p>
<p>　　connection.max.idle.ms:默认9分钟，多久后关闭限制的连接。</p>
<p>　　Linger.ms:这个参数是指ProducerBatch（看后文）等待更多ProducerRecord加入的时间，默认为0，生产者会在这个ProducerBatch被填满，或者等待时间超过Linger.ms时发送出去</p>
<p>　　send.buffer.bytes：socket接收消息缓冲区大小。默认32k。<br>　　<br>　　send.buffer.bytes：socket发送消息缓冲区大小。默认128k。</p>
<p>　　request.timeout.ms:等待请求响应最长时间，默认30000ms，超时后可以选择重试。但是注意了，这个值要比broker端的replica.lag.time.max.ms的值要大，可以减小因为客户端重试导致的消息重复。</p>
<h2 id="代码拼写"><a href="#代码拼写" class="headerlink" title="代码拼写"></a>代码拼写</h2><pre><code>props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
</code></pre><p>　　我们写这些参数时可能某个字符写错了。因此可以用ProducerConfig来替代。<br>　　KafkaProducer是线程安全的。</p>
<h2 id="发送消息"><a href="#发送消息" class="headerlink" title="发送消息"></a>发送消息</h2><p>　　３个模式<br>　　１．发后即忘<br>　　这个模式不关心消息是否正确到达，大多数也没什么问题，但是存在不可重试异常时，会造成消息的丢失，这个方式效率最高，可靠性最低。实现方式为</p>
<pre><code>try {
        producer.send(record);
    } catch (Exception e) {
        e.printStackTrace();
    }
</code></pre><p>　　２．同步<br>　　这个模式是直接链式调用send方法返回对象的get方法来阻塞等待kafka的响应。直到消息发送成功，或者发生异常被捕捉处理。实现方式如下：</p>
<pre><code>try {
        Future&lt;RecordMetadata&gt; future=producer.send(record);
        RecordMetadata recordMetadata=future.get();
    } catch (Exception e) {
        e.printStackTrace();
    }
</code></pre><p>　　３．异步<br>　　这个解决了上面的性能问题，上面的future也是异步的逻辑处理。但是写法没有下面的方便：</p>
<pre><code>producer.send(record, new Callback() {
            @Override
            public void onCompletion(RecordMetadata metadata, Exception exception) {
                if (exception != null)
                    exception.printStackTrace();
            }
        });
</code></pre><p>　　这样就不会阻塞程序的执行了，等道kafka响应时就会做出相关操作处理。前面说过kafka通过偏移量保证顺序消费，响应如果是同一个分区，那么也是顺序的。</p>
<p>　　close方法，会阻塞之前所有请求后再关闭KafkaProducer。来进行资源回收。</p>
<h2 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h2><p>　　除了上面说的StringDeserializer这个string类型的序列化器之外，还有Integer,Long,Double,Bytes,都实现了序列化接口，主要有2个方法，解析下：</p>
<pre><code>@Override
public void configure(Map&lt;String, ?&gt; configs, boolean isKey) {
    String propertyName = isKey ? &quot;key.serializer.encoding&quot; : &quot;value.serializer.encoding&quot;;
    Object encodingValue = configs.get(propertyName);
    if (encodingValue == null)
        encodingValue = configs.get(&quot;serializer.encoding&quot;);
    if (encodingValue instanceof String)
        encoding = (String) encodingValue;
}
</code></pre><p>　　这个方法是KafkaProducer创建实例时调用的，来确定编码类型。</p>
<pre><code>@Override
public byte[] serialize(String topic, String data) {
    try {
        if (data == null)
            return null;
        else
            return data.getBytes(encoding);
    } catch (UnsupportedEncodingException e) {
        throw new SerializationException(&quot;Error when serializing string to byte[] due to unsupported encoding &quot; + encoding);
    }
}
</code></pre><p>　　很简单，序列化。<br>　　我们也可以自定义序列化器。只需要实现org.apache.kafka.common.serialization.Serializer<t>接口即可。然后替换下即可。</t></p>
<h2 id="分区器"><a href="#分区器" class="headerlink" title="分区器"></a>分区器</h2><p>　　生成者send数据到broke时，需要经过拦截器，序列化器，和分区器等，在消息对象的partition不为空时，则不走分区器，为空则需要走分区器，分区器根据key来进行分区。<br>　　默认的分区器是DefaultPartitioner，实现了Partitioner这个接口。在这个类里面主要方法。</p>
<pre><code>public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {
    List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);
    int numPartitions = partitions.size();
    if (keyBytes == null) {
        int nextValue = nextValue(topic);
        List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic);
        if (availablePartitions.size() &gt; 0) {
            int part = Utils.toPositive(nextValue) % availablePartitions.size();
            return availablePartitions.get(part).partition();
        } else {
            // no partitions are available, give a non-available partition
            return Utils.toPositive(nextValue) % numPartitions;
        }
    } else {
        // hash the keyBytes to choose a partition
        return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;
    }
}
</code></pre><p>　　<br>　　代码很简单，key不为空时根据hash算法，算出key对应的编号。为空时则是获取这个topic的所有分区轮训。中间用到了currenthashmap<br>　　我们也可以通过实现Partitioner这个接口来自定义分区器，只需要早参数配置时加入</p>
<pre><code>props.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, DefaultPartitioner.class.getName());
</code></pre><p>　　来设定你的分区器。</p>
<h2 id="生产者拦截器"><a href="#生产者拦截器" class="headerlink" title="生产者拦截器"></a>生产者拦截器</h2><p>　　需要实现ProducerInterceptor这个接口即可。KafkaProducer的send方法</p>
<pre><code>@Override
public Future&lt;RecordMetadata&gt; send(ProducerRecord&lt;K, V&gt; record, Callback callback) {
    // intercept the record, which can be potentially modified; this method does not throw exceptions
    ProducerRecord&lt;K, V&gt; interceptedRecord = this.interceptors.onSend(record);
    return doSend(interceptedRecord, callback);
}
</code></pre><p>　　会在第一时间先调用拦截器的send方法来对消息进行相应的处理。</p>
<p>　　同时会在消息应答或者消息发送失败时，调用拦截器onAcknowledgement方法。这个调用在Callback之前发生。</p>
<pre><code>public class ProduceInterceptor implements ProducerInterceptor&lt;String, String&gt; {
@Override
public void configure(Map&lt;String, ?&gt; configs) {
    // TODO Auto-generated method stub
}
@Override
public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) {
    return record;
}
@Override
public void onAcknowledgement(RecordMetadata metadata, Exception exception) {
    // TODO Auto-generated method stub

}
@Override
public void close() {
    // TODO Auto-generated method stub
}
}
</code></pre><p>　　简单如上实现即可。　<br>　　然后在prop上注册即可。</p>
<pre><code>props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, ProduceInterceptor.class.getName());
</code></pre><p>　　同理，这边也支持职责链模式。也就是多个拦截器。</p>
<pre><code>props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, ProduceInterceptor.class.getName()+&quot;,&quot;+ProduceInterceptor.class.getName());
</code></pre><h2 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h2><p>　　１.主线程生产者调用send()方法后，先经过拦截器，序列化器和分区器后，在缓存到RecordAccumulator（消息累加器）中。<br>　　２.Send线程负责从RecordAccumulator获取消息，批量发送，减少网络传输的资源消耗。<br>　　３．有2个生产者参数，buffer.memory 这个是RecordAccumulator的大小默认是32m， 如果1的速度大于2，那么可能空间不足，因此1要么阻塞，要么抛出异常，取决于max.block.ms的配置，这个值默认是60s。<br>　　４.RecordAccumulator内部为每个分区维护了一个双向队列。ConcurrentMap<topicpartition, deque<producerbatch="">&gt; batches，写入消息是追加到尾部，send读消息时从头部读取，这里面的ProducerBatch是一个或者多个ProducerRecord的合成。<br>　　５.当一个消息ProducerRecord被append到RecordAccumulator时会根据一个叫做batch.size默认参数是16kb进行区分，首先判断尾部的ProducerBatch是否可以继续添加ProducerRecord，不可以的话，判断是否大于batch.size,如果不大于则按照16kb的大小创建ProducerBatch，因为RecordAccumulator内部有个BufferPool这个是缓存了固定大小的ByteBuffer对象。<br>　　６.InFlightRequests,在send线程发送到kafka之前，会把对象保存到这个对象里面Map<string, deque<networkclient.inflightrequest="">&gt;，这个里面request保存了发出去了，但是没有收到回复的响应请求。<br>　　７.元数据的更新<br>　　当客户端没有所需要的元数据时，就会向其中一个node发送请求，获取元数据（元数据包括：集群有哪些主题，哪些分区，lead副本在哪个节点上等）。</string,></topicpartition,></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://skydh.github.io/2019/06/19/springboot单应用脚手架/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="skydh">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="learning, progress, future.">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/19/springboot单应用脚手架/" itemprop="url">springboot单应用脚手架</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-19T16:32:50+08:00">
                2019-06-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="地址"><a href="#地址" class="headerlink" title="地址"></a>地址</h2><p> 　　<a href="https://github.com/skydh/springboot-scaffold" target="_blank" rel="noopener">脚手架地址</a></p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><h3 id="多数据源"><a href="#多数据源" class="headerlink" title="多数据源"></a>多数据源</h3><p>　　在configuration包里面，我们定义了2个数据源，一个是pgsql，一个是mysql。这边对mysql加了@Primary优先处理，因此mysql可以使用jpa进行操作，而pg由于业务只用其进行批量处理，因此只让其使用springjdbc即可。</p>
<p>　　由于jpa对批量处理以及复杂sql的不友好，因此这边建议，简单sql使用jpa，加快开发效率，复杂批量sql使用springjdbc，开发简单高效。我们约定，jpa的dao命名为xxxJpaDao,springjdbc的dao命名为xxxDao。</p>
<h3 id="aop动态代理"><a href="#aop动态代理" class="headerlink" title="aop动态代理"></a>aop动态代理</h3><p>　　1.非法字符过滤，这个功能主要是检查你的rest接口里面的vo是否存在非法字符，会递归检查到基本类型，但是要检验的vo必须实现一个空接口。支持list，vo,map这3个数据混搭的检查。</p>
<p>　　2.任务调度加锁。这个功能主要是用于工程在集群部署的情况下。定时器多次调用做了加锁限制。使用的是redis分布式锁。</p>
<p>　　3.数据字段校验，这个主要是使用spring的安全校验来检查表单字段。</p>
<p>　　4.ip校验，控制这个游客接口最多访问次数。</p>
<h3 id="上下文"><a href="#上下文" class="headerlink" title="上下文"></a>上下文</h3><p>　　上下文是用ThreadLocal实现的，默认实现了用户信息的缓存，同时开发也可以自定义往里面丢值进去，但是请记住，我们访问服务的线程是从线程池里面取出来的，因此这个线程里面可能存在旧的值，因此，你需要在代码里面控制，要么每次都覆盖，要么在这个线程或者这个请求结束前将其数据释放。默认使用覆盖策略。</p>
<h3 id="枚举"><a href="#枚举" class="headerlink" title="枚举"></a>枚举</h3><p>　　常量一律使用枚举。</p>
<h3 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h3><p>　　使用了全局异常捕捉。</p>
<h2 id="http"><a href="#http" class="headerlink" title="http"></a>http</h2><p>　　由于是单应用，存在远程调用，为了方便，我们使用http方式调用，采用的是spring的RestTemplate作为接口，httpClient为实现的方式。</p>
<h2 id="拦截器"><a href="#拦截器" class="headerlink" title="拦截器"></a>拦截器</h2><p>　　拦截器有２个，一个是登录校验的拦截器，一个是基本非法字符校验的拦截器。</p>
<h2 id="国际化"><a href="#国际化" class="headerlink" title="国际化"></a>国际化</h2><p>　　返回信息需要国际化处理，因为可能是不同国家，需要不同的语言。</p>
<h2 id="分页查询"><a href="#分页查询" class="headerlink" title="分页查询"></a>分页查询</h2><p>　　基于springjdbc封装了一个分页查询的工具类。ps:可以考虑封装更多的工具类来完全代替jpa。</p>
<h2 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h2><p>　　定义了２个事务管理器，一个是pgsql的DataSourceTransactionManager，来管理pgsql的springjdbc的事务，一个是JpaTransactionManager的事务管理器，可以管理mysql的jpa和spring jdbc的事务管理。</p>
<p>　　ps(This transaction manager also supports direct DataSource access within a transaction<br>(i.e. plain JDBC code working with the same DataSource).<br> This allows for mixing services which access JPA and services which use plain JDBC (without being aware of JPA)!<br> Application code needs to stick to the same simple Connection lookup pattern as with DataSourceTransactionManager<br> (i.e. DataSourceUtils.getConnection(javax.sql.DataSource) or going through a TransactionAwareDataSourceProxy).<br>  Note that this requires a vendor-specific JpaDialect to be configured.)</p>
<h2 id="返回值"><a href="#返回值" class="headerlink" title="返回值"></a>返回值</h2><p>　　返回值信息有４个，分别是，是否成功，返回code,返回message,返回vo。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">skydh</p>
              <p class="site-description motion-element" itemprop="description">skydh</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">127</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          

          <div class="links-of-author motion-element">
            
          </div>

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">skydh</span>

  
</div>


<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  本站访客数:<span id="busuanzi_value_site_uv"></span>
</span>
</div>

  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.3</div>




        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
