<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="clear 航">
<meta property="og:type" content="website">
<meta property="og:title" content="why are you so happy">
<meta property="og:url" content="https://skydh.github.io/index.html">
<meta property="og:site_name" content="why are you so happy">
<meta property="og:description" content="clear 航">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="why are you so happy">
<meta name="twitter:description" content="clear 航">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://skydh.github.io/"/>





  <title>why are you so happy</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">why are you so happy</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">clear 航</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://skydh.github.io/2019/06/25/kafka基础-消费者/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="董航">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="why are you so happy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/25/kafka基础-消费者/" itemprop="url">kafak入门003 -消费者</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-25T16:24:53+08:00">
                2019-06-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://skydh.github.io/2019/06/24/kafka集群配置/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="董航">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="why are you so happy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/24/kafka集群配置/" itemprop="url">kafak入门004 -集群配置</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-24T14:20:55+08:00">
                2019-06-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="配置存储的参数"><a href="#配置存储的参数" class="headerlink" title="配置存储的参数"></a>配置存储的参数</h2><p>　　log.dirs：这是非常重要的参数，指定了Broker需要使用的若干个文件目录路径。这个参数是没有默认值的，必须由你亲自指定。样例：比如/home/kafka1,/home/kafka2,/home/kafka3这样。如果有条件的话你最好保证这些目录挂载到不同的物理磁盘上。多个磁盘可以提高读写性能，实现故障转移。<br>　　log.dir：能表示单个路径，它是补充上一个参数用的。一般不需要设置</p>
<h2 id="zookeeper参数"><a href="#zookeeper参数" class="headerlink" title="zookeeper参数"></a>zookeeper参数</h2><p>　　zookeeper.connect：比如我可以指定它的值为zk1:2181,zk2:2181,zk3:2181。2181是ZooKeeper的默认端口如果多个kafka集群共用一个zookeeper集群时，如何配置处理。在最后的一个zookeeper后加个别名例如：zk1:2181,zk2:2181,zk3:2181/kafka</p>
<h2 id="broker通信相关"><a href="#broker通信相关" class="headerlink" title="broker通信相关"></a>broker通信相关</h2><p>　　listeners这个配置的，前文由于写demo介绍过。</p>
<p>　　listeners：学名叫监听器，其实就是告诉外部连接者要通过什么协议访问指定主机名和端口开放的Kafka服务。<br>　　advertised.listeners：和listeners相比多了个advertised。Advertised的含义表示宣称的、公布的，就是说这组监听器是Broker用于对外发布的。<br>　　host.name/port：列出这两个参数就是想说你把它们忘掉吧，压根不要为它们指定值，毕竟都是过期的参数了。</p>
<h2 id="topic"><a href="#topic" class="headerlink" title="topic"></a>topic</h2><p>　　auto.create.topics.enable：是否允许自动创建Topic。一般为false，让运维管控。免得出现一堆奇葩命名的<br>　　unclean.leader.election.enable：是否允许Unclean Leader选举。每个分区存在多个副本，有的副本落后lead太多数据，但是lead和其他不落后的都挂了，如果为true，那么这个分区可能数据丢失，因为会选举落后很多的副本作为lead<br>　　auto.leader.rebalance.enable：是否允许定期进行Leader选举。它的值为true表示允许Kafka定期地对一些Topic分区进行Leader重选举，这个是强行选举，因此需要设置为false.</p>
<h2 id="数据留存"><a href="#数据留存" class="headerlink" title="数据留存"></a>数据留存</h2><p>　　log.retention.{hour|minutes|ms}：这是个“三兄弟”，都是控制一条消息数据被保存多长时间。从优先级上来说ms设置最高、minutes次之、hour最低。<br>　　log.retention.bytes：这是指定Broker为消息保存的总磁盘容量大小。<br>　　message.max.bytes：控制Broker能够接收的最大消息大小</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://skydh.github.io/2019/06/20/kafka基础-生产者/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="董航">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="why are you so happy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/20/kafka基础-生产者/" itemprop="url">kafak入门002 -生产者</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-20T10:30:55+08:00">
                2019-06-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="消息格式"><a href="#消息格式" class="headerlink" title="消息格式"></a>消息格式</h2><pre><code>public class ProducerRecord&lt;K, V&gt; {
    private final String topic;
    private final Integer partition;
    private final Headers headers;
    private final K key;
    private final V value;
    private final Long timestamp;
}
</code></pre><p>　　topic是消息主题，partition是分区号，headers一般没用，key是消息的键，同一个key的消息会被发到一个分区里面。</p>
<h2 id="参数配置"><a href="#参数配置" class="headerlink" title="参数配置"></a>参数配置</h2><p>　　bootstrap.servers：生成者客户端连接kafka集群所需的broker地址清单，一般写2个，host:port,host1:port1,不要全部写出来，因为生产者会从给定的broke里面查出其他broker的信息，写2个是为了防止一个宕机了，生产者依旧可以连接到kafak。</p>
<p>　　key.deserializer，value.deserializer：broker服务器接受的消息是字节数组存在的，这个就是指定，key和value的序列化操作的序列化器。</p>
<p>　　client.id:对应kafka客户端的id，也就是客户端的名字，如果不设置则是默认取名，produce-i</p>
<p>　　retries：对于可重试异常，可重试次数配置，默认是０，ps(KafkaProducer一般有2种异常，一个是可重试异常，比如NetworkException,表示网络异常，可以通过重试解决，一个是不可重试异常，比如RecordToolargeException,表示消息太大，不会重试。)，重试次数超过了配置的值。也会抛出异常。<br>　　retry.backoff.ms：这个主要是和上面的retries相对应的。是重试时间间隔。默认是100</p>
<p>　　props.put(ProducerConfig.ACKS_CONFIG, “0”);<br>　　当为0时，发到服务端就不管了。<br>　　当为1时，只要leader副本写入，就返回，但是可能leader奔溃了，但是其他副本没有拉取到消息。造成数据丢失<br>　　当为-1或者all时，必须所有副本都要同步完才返回。</p>
<p>　　max.request.size:默认值是1m,主要要和broke端配置的message.max.bytes相对应。</p>
<p>　　max.in.flight.requests.per.connection为1时，可以保证顺序消费。<br>　　compression.type:默认为none，可以配置为gzip,lz4等压缩方式，可以减小网络io但是会造成时延。</p>
<p>　　connection.max.idle.ms:默认9分钟，多久后关闭限制的连接。</p>
<p>　　Linger.ms:这个参数是指ProducerBatch（看后文）等待更多ProducerRecord加入的时间，默认为0，生产者会在这个ProducerBatch被填满，或者等待时间超过Linger.ms时发送出去</p>
<p>　　send.buffer.bytes：socket接收消息缓冲区大小。默认32k。<br>　　<br>　　send.buffer.bytes：socket发送消息缓冲区大小。默认128k。</p>
<p>　　request.timeout.ms:等待请求响应最长时间，默认30000ms，超时后可以选择重试。但是注意了，这个值要比broker端的replica.lag.time.max.ms的值要大，可以减小因为客户端重试导致的消息重复。</p>
<h2 id="代码拼写"><a href="#代码拼写" class="headerlink" title="代码拼写"></a>代码拼写</h2><pre><code>props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
</code></pre><p>　　我们写这些参数时可能某个字符写错了。因此可以用ProducerConfig来替代。<br>　　KafkaProducer是线程安全的。</p>
<h2 id="发送消息"><a href="#发送消息" class="headerlink" title="发送消息"></a>发送消息</h2><p>　　３个模式<br>　　１．发后即忘<br>　　这个模式不关心消息是否正确到达，大多数也没什么问题，但是存在不可重试异常时，会造成消息的丢失，这个方式效率最高，可靠性最低。实现方式为</p>
<pre><code>try {
        producer.send(record);
    } catch (Exception e) {
        e.printStackTrace();
    }
</code></pre><p>　　２．同步<br>　　这个模式是直接链式调用send方法返回对象的get方法来阻塞等待kafka的响应。直到消息发送成功，或者发生异常被捕捉处理。实现方式如下：</p>
<pre><code>try {
        Future&lt;RecordMetadata&gt; future=producer.send(record);
        RecordMetadata recordMetadata=future.get();
    } catch (Exception e) {
        e.printStackTrace();
    }
</code></pre><p>　　３．异步<br>　　这个解决了上面的性能问题，上面的future也是异步的逻辑处理。但是写法没有下面的方便：</p>
<pre><code>producer.send(record, new Callback() {
            @Override
            public void onCompletion(RecordMetadata metadata, Exception exception) {
                if (exception != null)
                    exception.printStackTrace();
            }
        });
</code></pre><p>　　这样就不会阻塞程序的执行了，等道kafka响应时就会做出相关操作处理。前面说过kafka通过偏移量保证顺序消费，响应如果是同一个分区，那么也是顺序的。</p>
<p>　　close方法，会阻塞之前所有请求后再关闭KafkaProducer。来进行资源回收。</p>
<h2 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h2><p>　　除了上面说的StringDeserializer这个string类型的序列化器之外，还有Integer,Long,Double,Bytes,都实现了序列化接口，主要有2个方法，解析下：</p>
<pre><code>@Override
public void configure(Map&lt;String, ?&gt; configs, boolean isKey) {
    String propertyName = isKey ? &quot;key.serializer.encoding&quot; : &quot;value.serializer.encoding&quot;;
    Object encodingValue = configs.get(propertyName);
    if (encodingValue == null)
        encodingValue = configs.get(&quot;serializer.encoding&quot;);
    if (encodingValue instanceof String)
        encoding = (String) encodingValue;
}
</code></pre><p>　　这个方法是KafkaProducer创建实例时调用的，来确定编码类型。</p>
<pre><code>@Override
public byte[] serialize(String topic, String data) {
    try {
        if (data == null)
            return null;
        else
            return data.getBytes(encoding);
    } catch (UnsupportedEncodingException e) {
        throw new SerializationException(&quot;Error when serializing string to byte[] due to unsupported encoding &quot; + encoding);
    }
}
</code></pre><p>　　很简单，序列化。<br>　　我们也可以自定义序列化器。只需要实现org.apache.kafka.common.serialization.Serializer<t>接口即可。然后替换下即可。</t></p>
<h2 id="分区器"><a href="#分区器" class="headerlink" title="分区器"></a>分区器</h2><p>　　生成者send数据到broke时，需要经过拦截器，序列化器，和分区器等，在消息对象的partition不为空时，则不走分区器，为空则需要走分区器，分区器根据key来进行分区。<br>　　默认的分区器是DefaultPartitioner，实现了Partitioner这个接口。在这个类里面主要方法。</p>
<pre><code>public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {
    List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);
    int numPartitions = partitions.size();
    if (keyBytes == null) {
        int nextValue = nextValue(topic);
        List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic);
        if (availablePartitions.size() &gt; 0) {
            int part = Utils.toPositive(nextValue) % availablePartitions.size();
            return availablePartitions.get(part).partition();
        } else {
            // no partitions are available, give a non-available partition
            return Utils.toPositive(nextValue) % numPartitions;
        }
    } else {
        // hash the keyBytes to choose a partition
        return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;
    }
}
</code></pre><p>　　<br>　　代码很简单，key不为空时根据hash算法，算出key对应的编号。为空时则是获取这个topic的所有分区轮训。中间用到了currenthashmap<br>　　我们也可以通过实现Partitioner这个接口来自定义分区器，只需要早参数配置时加入</p>
<pre><code>props.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, DefaultPartitioner.class.getName());
</code></pre><p>　　来设定你的分区器。</p>
<h2 id="生产者拦截器"><a href="#生产者拦截器" class="headerlink" title="生产者拦截器"></a>生产者拦截器</h2><p>　　需要实现ProducerInterceptor这个接口即可。KafkaProducer的send方法</p>
<pre><code>@Override
public Future&lt;RecordMetadata&gt; send(ProducerRecord&lt;K, V&gt; record, Callback callback) {
    // intercept the record, which can be potentially modified; this method does not throw exceptions
    ProducerRecord&lt;K, V&gt; interceptedRecord = this.interceptors.onSend(record);
    return doSend(interceptedRecord, callback);
}
</code></pre><p>　　会在第一时间先调用拦截器的send方法来对消息进行相应的处理。</p>
<p>　　同时会在消息应答或者消息发送失败时，调用拦截器onAcknowledgement方法。这个调用在Callback之前发生。</p>
<pre><code>public class ProduceInterceptor implements ProducerInterceptor&lt;String, String&gt; {
@Override
public void configure(Map&lt;String, ?&gt; configs) {
    // TODO Auto-generated method stub
}
@Override
public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) {
    return record;
}
@Override
public void onAcknowledgement(RecordMetadata metadata, Exception exception) {
    // TODO Auto-generated method stub

}
@Override
public void close() {
    // TODO Auto-generated method stub
}
}
</code></pre><p>　　简单如上实现即可。　<br>　　然后在prop上注册即可。</p>
<pre><code>props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, ProduceInterceptor.class.getName());
</code></pre><p>　　同理，这边也支持职责链模式。也就是多个拦截器。</p>
<pre><code>props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, ProduceInterceptor.class.getName()+&quot;,&quot;+ProduceInterceptor.class.getName());
</code></pre><h2 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h2><p>　　１.主线程生产者调用send()方法后，先经过拦截器，序列化器和分区器后，在缓存到RecordAccumulator（消息累加器）中。<br>　　２.Send线程负责从RecordAccumulator获取消息，批量发送，减少网络传输的资源消耗。<br>　　３．有2个生产者参数，buffer.memory 这个是RecordAccumulator的大小默认是32m， 如果1的速度大于2，那么可能空间不足，因此1要么阻塞，要么抛出异常，取决于max.block.ms的配置，这个值默认是60s。<br>　　４.RecordAccumulator内部为每个分区维护了一个双向队列。ConcurrentMap<topicpartition, deque<producerbatch="">&gt; batches，写入消息是追加到尾部，send读消息时从头部读取，这里面的ProducerBatch是一个或者多个ProducerRecord的合成。<br>　　５.当一个消息ProducerRecord被append到RecordAccumulator时会根据一个叫做batch.size默认参数是16kb进行区分，首先判断尾部的ProducerBatch是否可以继续添加ProducerRecord，不可以的话，判断是否大于batch.size,如果不大于则按照16kb的大小创建ProducerBatch，因为RecordAccumulator内部有个BufferPool这个是缓存了固定大小的ByteBuffer对象。<br>　　６.InFlightRequests,在send线程发送到kafka之前，会把对象保存到这个对象里面Map<string, deque<networkclient.inflightrequest="">&gt;，这个里面request保存了发出去了，但是没有收到回复的响应请求。<br>　　７.元数据的更新<br>　　当客户端没有所需要的元数据时，就会向其中一个node发送请求，获取元数据（元数据包括：集群有哪些主题，哪些分区，lead副本在哪个节点上等）</string,></topicpartition,></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://skydh.github.io/2019/06/19/springboot单应用脚手架/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="董航">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="why are you so happy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/19/springboot单应用脚手架/" itemprop="url">springboot单应用脚手架</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-19T16:32:50+08:00">
                2019-06-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="地址"><a href="#地址" class="headerlink" title="地址"></a>地址</h2><p> 　　<a href="https://github.com/skydh/springboot-scaffold" target="_blank" rel="noopener">脚手架地址</a></p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><h3 id="多数据源"><a href="#多数据源" class="headerlink" title="多数据源"></a>多数据源</h3><p>　　在configuration包里面，我们定义了2个数据源，一个是pgsql，一个是mysql。这边对mysql加了@Primary优先处理，因此mysql可以使用jpa进行操作，而pg由于业务只用其进行批量处理，因此只让其使用springjdbc即可。</p>
<p>　　由于jpa对批量处理以及复杂sql的不友好，因此这边建议，简单sql使用jpa，加快开发效率，复杂批量sql使用springjdbc，开发简单高效。我们约定，jpa的dao命名为xxxJpaDao,springjdbc的dao命名为xxxDao。</p>
<h3 id="aop动态代理"><a href="#aop动态代理" class="headerlink" title="aop动态代理"></a>aop动态代理</h3><p>　　1.非法字符过滤，这个功能主要是检查你的rest接口里面的vo是否存在非法字符，会递归检查到基本类型，但是要检验的vo必须实现一个空接口。支持list，vo,map这3个数据混搭的检查。</p>
<p>　　2.任务调度加锁。这个功能主要是用于工程在集群部署的情况下。定时器多次调用做了加锁限制。使用的是redis分布式锁。</p>
<p>　　3.数据字段校验，这个主要是使用spring的安全校验来检查表单字段。</p>
<p>　　4.ip校验，控制这个游客接口最多访问次数。</p>
<h3 id="上下文"><a href="#上下文" class="headerlink" title="上下文"></a>上下文</h3><p>　　上下文是用ThreadLocal实现的，默认实现了用户信息的缓存，同时开发也可以自定义往里面丢值进去，但是请记住，我们访问服务的线程是从线程池里面取出来的，因此这个线程里面可能存在旧的值，因此，你需要在代码里面控制，要么每次都覆盖，要么在这个线程或者这个请求结束前将其数据释放。默认使用覆盖策略。</p>
<h3 id="枚举"><a href="#枚举" class="headerlink" title="枚举"></a>枚举</h3><p>　　常量一律使用枚举。</p>
<h3 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h3><p>　　使用了全局异常捕捉。</p>
<h2 id="http"><a href="#http" class="headerlink" title="http"></a>http</h2><p>　　由于是单应用，存在远程调用，为了方便，我们使用http方式调用，采用的是spring的RestTemplate作为接口，httpClient为实现的方式。</p>
<h2 id="拦截器"><a href="#拦截器" class="headerlink" title="拦截器"></a>拦截器</h2><p>　　拦截器有２个，一个是登录校验的拦截器，一个是基本非法字符校验的拦截器。</p>
<h2 id="国际化"><a href="#国际化" class="headerlink" title="国际化"></a>国际化</h2><p>　　返回信息需要国际化处理，因为可能是不同国家，需要不同的语言。</p>
<h2 id="分页查询"><a href="#分页查询" class="headerlink" title="分页查询"></a>分页查询</h2><p>　　基于springjdbc封装了一个分页查询的工具类。ps:可以考虑封装更多的工具类来完全代替jpa。</p>
<h2 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h2><p>　　定义了２个事务管理器，一个是pgsql的DataSourceTransactionManager，来管理pgsql的springjdbc的事务，一个是JpaTransactionManager的事务管理器，可以管理mysql的jpa和spring jdbc的事务管理。</p>
<p>　　ps(This transaction manager also supports direct DataSource access within a transaction<br>(i.e. plain JDBC code working with the same DataSource).<br> This allows for mixing services which access JPA and services which use plain JDBC (without being aware of JPA)!<br> Application code needs to stick to the same simple Connection lookup pattern as with DataSourceTransactionManager<br> (i.e. DataSourceUtils.getConnection(javax.sql.DataSource) or going through a TransactionAwareDataSourceProxy).<br>  Note that this requires a vendor-specific JpaDialect to be configured.)</p>
<h2 id="返回值"><a href="#返回值" class="headerlink" title="返回值"></a>返回值</h2><p>　　返回值信息有４个，分别是，是否成功，返回code,返回message,返回vo。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://skydh.github.io/2019/06/18/HikariCP-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="董航">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="why are you so happy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/18/HikariCP-1/" itemprop="url">HikariCP 笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-18T10:03:42+08:00">
                2019-06-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h2><p>　　１．HikariCp优化创建了concurrentBag，增加了并发读写的效率，减少了其他没必要的操作，是的集合性能提高了很多。<br>　　２．使用threadlocal缓存连接以及大量使用cas的机制，避免lock.<br>　　３．使用Javassist来实现动态代理类，实现了字节码的优化。<br>　　４．心跳语句由select 1 变成了ping。<br>　　５．大量的方法内联。效率提高了很多。<br>　　６．代码量少。<br>　　７．FastStatementList代替了Arraylist，去掉了调用get时的范围检查，增加性能。</p>
<h2 id="声明定义"><a href="#声明定义" class="headerlink" title="声明定义"></a>声明定义</h2><p>　　HikariDataSource：数据源，我们通过其对象来获取连接。</p>
<p>　　HikariPool：连接池，对资源进行管理。</p>
<p>　　ConcurrentBag：作为物理连接的共享资源站。</p>
<p>　　PoolEntry：物理连接的封装。</p>
<p> 　　获取连接流程getConnection：<br>　　１．HikariDataSource对象调用getConnection来获取连接<br>　　２．调用HikariPool的getConnection获取连接。<br>　　３．调用ConcurrentBag取出一个PoolEntry．然后这个PoolEntry通过createProxyConnection掉用工厂类生成HikariProxyConnection返回。</p>
<p>　　连接关闭closeConnection()：<br>　　closeConnectionExecutor关闭连接后，会调用fillPool()方法对连接池进行连接填充。同时HikariPool提供evictConnection(Connection)方法对物理连接进行手动关闭。<br>　　连接关闭close():<br>　　HikariProxyConnection调用close方法时调用了PooleEntry的recycle方法，之后通过HikariPool调用了ConcurrentBag的requite放回。（poolEntry通过borrow从bag中取出，再通过requite放回。资源成功回收）。</p>
<p>　　创建连接createPoolEntry：<br>　　HikariCP中通过独立的线程池addConnectionExecutor进行新连接的生成，连接生成方法为PoolEntryCreator。物理链接的生成只由PoolBase的newConnection()实现，之后封装成PoolEntry，通过Bag的add方法加入ConcurrentBag。当ConcurrentBag存在等待线程，或者有连接被关闭时，会触发IBagItemListener的addBagItem(wait)方法，调用PoolEntryCreator进行新连接的生成。</p>
<h2 id="连接池到底该多大"><a href="#连接池到底该多大" class="headerlink" title="连接池到底该多大"></a>连接池到底该多大</h2><p>　　２个参数：</p>
<p>　　maxPoolSize：最大连接数。</p>
<p>　　minIdle：最小连接数。作者建议这个值和maxPoolSize保持一致作为一个固定大小的连接池。</p>
<p>　　连接池大家是综合每个应用系统的业务逻辑特性，加上应用硬件配置，加上应用部署数量，再加上db硬件配置和最大允许连接数测试出来的。很难有一个简单公式进行计算。连接数及超时时间设置不正确经常会带来较大的性能问题，并影响整个服务能力的稳定性。具体设置多少，要看系统的访问量，可通过反复测试，找到最佳点。压测很重要。</p>
<p>　　我们进行判断时，可以从以下几个层面分析：<br>　　系统中多少个线程在进行与数据库有关的工作？多少个线程在等待获取数据库连接？获取数据库连接需要的平均时长是多少？如果平均时长较长，如大于 100ms，则可能说明配置的数据库连接数不足，或存在连接泄漏问题。
　　</p>
<p>　　
　　</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://skydh.github.io/2019/06/03/kafaka基本概念入门/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="董航">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="why are you so happy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/03/kafaka基本概念入门/" itemprop="url">kafak入门001 -基本概念</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-03T18:37:43+08:00">
                2019-06-03
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>读书笔记系列，一章一篇。<br>取自深入理解kafka和kafka专栏（极客时间）</p>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><h4 id="消息系统"><a href="#消息系统" class="headerlink" title="消息系统"></a>消息系统</h4><p>　　和传统的消息系统一样，但是额外提供，顺序消费，回溯消费。</p>
<h4 id="存储系统"><a href="#存储系统" class="headerlink" title="存储系统"></a>存储系统</h4><p>　　消息是持久化存储的。</p>
<h3 id="流式平台处理"><a href="#流式平台处理" class="headerlink" title="流式平台处理"></a>流式平台处理</h3><p>　　kafka不仅为多个流行的流式处理框架提供了可靠的数据来源，还提供了完整的流式处理类库。</p>
<h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><h3 id="producer"><a href="#producer" class="headerlink" title="producer"></a>producer</h3><p>　　生产者：生产消息发送给kakfa实例。</p>
<h3 id="Consumer"><a href="#Consumer" class="headerlink" title="Consumer"></a>Consumer</h3><p>　　消费者：负责消费kafka上面的消息。消费者端采用pull的方式拉去消息，保存了数据的偏移量，当消费者宕机后重新上线，可以更具之前保存的消费位置重新进行消费。<br>　　即点对点模型（Peer to Peer，P2P）和发布订阅模型。这里面的点对点指的是同一条消息只能被下游的一个消费者消费，其他消费者则不能染指。在Kafka中实现这种P2P模型的方法就是引入了消费者组（Consumer Group）。所谓的消费者组，指的是多个消费者实例共同组成一个组来消费一组主题。这组主题中的每个分区都只会被组内的一个消费者实例消费，其他消费者实例不能消费它。消费者组里面的所有消费者实例不仅“瓜分”订阅主题的数据，而且更酷的是它们还能彼此协助。假设组内某个实例挂掉了，Kafka能够自动检测到，然后把这个Failed实例之前负责的分区转移给其他活着的消费者。这个过程就是Kafka中大名鼎鼎的“重平衡”（Rebalance）。</p>
<h3 id="Broker"><a href="#Broker" class="headerlink" title="Broker"></a>Broker</h3><p>　　一个kafka实例，一个kafka集群里面有多个Broker，也就是多个kafka实例</p>
<h3 id="ZooKeeper"><a href="#ZooKeeper" class="headerlink" title="ZooKeeper"></a>ZooKeeper</h3><p>　　Zk负责管理kafka集群的的元数据，控制器的选举等。它是一个分布式协调框架，负责协调管理并保存Kafka集群的所有元数据信息，比如集群都有哪些Broker在运行、创建了哪些Topic，每个Topic都有多少分区以及这些分区的Leader副本都在哪些机器上等信息。</p>
<h3 id="主题"><a href="#主题" class="headerlink" title="主题"></a>主题</h3><p>　　kafka的消息是根据主题来分类。生产者发送的消息都要指定一个主题，消费者消费消息也是订阅主题。</p>
<h3 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h3><p>　　主题是逻辑上的概念，一个主题可能是多个分区组成的，一个分区只可能会是一个主题的。分区可以看做是一个可追加的日志文件，每个消息在分区里面都有一个偏移量，kafka通过这个偏移量保证顺序消费，但是kafka保证的是分区顺序消费，而不是主题顺序消费。</p>
<h3 id="副本"><a href="#副本" class="headerlink" title="副本"></a>副本</h3><p>　　一个分区有多个副本，其中一个是lead副本，其余是follow副本，分布在不同的broker里面，lead副本负责读写，其余副本负责和lead副本消息同步，当lead副本故障时，可以选举follow副本继续提供服务。由于副本的同步是异步的，因此可能不一致，所有副本统称为AR，所有和lead副本保持一致的是统称为ISR，与lead副本滞后的副本则统称为osr副本。这2个集合的副本是动态的，当ISR不满足时则将其提到OSR,当OSR更上时，则将其丢到ISR,副本数据的限制和你的当前borker相关，因为多个副本放在一台broken上毫无意思，因此，每个副本在不同的broken上。</p>
<h3 id="HW"><a href="#HW" class="headerlink" title="HW"></a>HW</h3><p>　　高水位。这个是消费者目前只能拉倒的消息偏移量。这个值是所有副本AR里面保存数据最小值offset。</p>
<h3 id="LEO"><a href="#LEO" class="headerlink" title="LEO"></a>LEO</h3><p>　　则是记录当前下一条数据写入的offset.</p>
<h3 id="版本号"><a href="#版本号" class="headerlink" title="版本号"></a>版本号</h3><p>　　kafka-2.11-2.1.1，这个里面2.11是scala的编译器版本，2.1.1是kafka真实的版本。2是大版本，第一个1是小版本号，第二个1是修订号。<br>　　0.7远古版本。<br>　　0.8加了副本机制，但是api是远古版本客户端api，且和新版本不兼容。<br>　　0.9加入了安全认证，权限的功能，新版本生产者api稳定，但是消费者api极其不稳定。<br>　　0.10:加了了kafka Streams。消费者api较为稳定。<br>　　0.11提供幂等性produceapi和事务api.以及对消息做了重构。<br>　　1.0和2.0对stream做了优化，消息没有什么变化。</p>
<h3 id="简单java代码"><a href="#简单java代码" class="headerlink" title="简单java代码"></a>简单java代码</h3><p>　　ps:注意kafka的server.properrties的listeners要开放出来，其格式为protocoll://hostname:port1。协议+域名ip+端口。第一个协议有很多，默认是PLAINTEXT不开启安全校验，域名要写出来，不写则是默认网卡，但是可能为127.0.0.1导致外部无法访问。还有advertised.listeners作用和listeners类似，主要用于IaaS环境，比如公有云上配备多个网卡，包含了私网网卡和公网网卡。对于这种情况，可以设置advertised.listeners绑定公网ip对外客户端使用。使用listeners绑定私网ip给broker之间通信。</p>
<p>　　maven依赖</p>
<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
    &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;
&lt;/dependency&gt;
</code></pre><p>　　生产者：</p>
<pre><code>package com.dh.kafka;
import java.util.Properties;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;
/**
 * 
 * @author Lenovo
  *
 */
public class SimpleKafkaProducer {
    public static void main(String[] args) {
        Properties props = new Properties();
        // broker地址
        props.put(&quot;bootstrap.servers&quot;, &quot;192.168.147.131:9092&quot;);
        // 指定消息key序列化方式
        props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);
        // 指定消息本身的序列化方式
        props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);
        KafkaProducer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props);
        ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(&quot;topic-demo111&quot;, &quot;hello, kafka&quot;);
        try {
            producer.send(record);
        } catch (Exception e) {
            e.printStackTrace();
        }
        System.out.println(&quot;Message sent successfully&quot;);
        producer.close();
    }
}
</code></pre><p>　　消费者：</p>
<pre><code>package com.dh.kafka;
import java.time.Duration;
import java.util.Collections;
import java.util.Properties;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
public class SimpleKafkaConsumer {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(&quot;bootstrap.servers&quot;, &quot;192.168.147.131:9092&quot;);
        // 每个消费者分配独立的组号
        props.put(&quot;group.id&quot;, &quot;group.demo&quot;);
        // 设置多久一次更新被消费消息的偏移量
        props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;);
        props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);
        props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);
        @SuppressWarnings(&quot;resource&quot;)
        KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);
        consumer.subscribe(Collections.singletonList(&quot;topic-demo111&quot;));
        System.out.println(&quot;Subscribed to topic &quot; + &quot;topic-demo111&quot;);
        while (true) {
            ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(1000));
            for (ConsumerRecord&lt;String, String&gt; record : records)
                System.out.printf(record.value());
        }
    }
}
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://skydh.github.io/2019/05/28/内部临时表/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="董航">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="why are you so happy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/28/内部临时表/" itemprop="url">mysql内部临时表</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-28T19:03:28+08:00">
                2019-05-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>　　在mysql里面，join_buffer，sort_buffer,内部临时表这3个都是在执行sql时暂存数据用的。前面2个前面文章都有介绍，本文主要介绍内部临时表。</p>
<h2 id="union"><a href="#union" class="headerlink" title="union"></a>union</h2><pre><code>(select 1000 as f) union (select id from t1 order by id desc limit 2);
</code></pre><p>　　比如这个sql，就是用到了内部临时表。这个sql就是查询2个子查询的并集，去重，若是不要去重，则是union all。<br>　　这个语句的操作流程是先创建一个临时表把第一个子查询的数据丢进去，然后再把第二个查询的数据丢进去。如果用union all则不涉及到去重，那么就不会创建临时表，只是直接拼装结果返回。</p>
<h2 id="group-by"><a href="#group-by" class="headerlink" title="group by"></a>group by</h2><p>　　１．select id%10 as m, count(*) as c from t1 group by m;<br>　　其操作流程为，先创建一个内部临时表，存放2个字段，m,c然后遍历主键，将计算出来的值丢到m，判断是否存在，若存在则+1，然后输出。</p>
<h2 id="大小参数"><a href="#大小参数" class="headerlink" title="大小参数"></a>大小参数</h2><p>　　默认16m，由这个参数tmp_table_size确认。</p>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化　"></a>优化　</h2><p>　　select SQL_BIG_RESULT id%100 as m, count(*) as c from t1 group by m;<br>　　如此，可以直接省去其中某些步骤，这是针对数据量大的情况直接执行走磁盘临时表。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://skydh.github.io/2019/05/28/maven一更新jdk版本就变化/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="董航">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="why are you so happy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/28/maven一更新jdk版本就变化/" itemprop="url">maven一更新jdk版本就变化</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-28T14:25:09+08:00">
                2019-05-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>　　每次更新maven，项目的jdk版本，编译版本就会变成1.5，很奇怪。</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>　　由于公司私服没有某个jar，只能切换为默认配置，从中央仓库下载后上传到私服，结果更新maven后，编译环境就变化。</p>
<h2 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h2><p>　　发现是setting文件出现问题，需要添加</p>
<pre><code>&lt;profile&gt;
        &lt;id&gt;jdk&lt;/id&gt;
        &lt;activation&gt;
            &lt;activeByDefault&gt;true&lt;/activeByDefault&gt;
            &lt;jdk&gt;1.8&lt;/jdk&gt;
        &lt;/activation&gt;
        &lt;properties&gt;
            &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;
            &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;
            &lt;maven.compiler.compilerVersion&gt;1.8&lt;/maven.compiler.compilerVersion&gt;
        &lt;/properties&gt;
    &lt;/profile&gt;
</code></pre><p>　　进入默认的配置文件即可。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://skydh.github.io/2019/05/27/eclipce 奇葩缺包异常/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="董航">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="why are you so happy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/27/eclipce 奇葩缺包异常/" itemprop="url">eclipse奇葩缺包异常</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-27T11:11:40+08:00">
                2019-05-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>　　早上一来，打开工程，直接报错Missing artifact jdk.tools:jdk.tools:jar:1.8，但是显然其他同事没有报错，网上搜索，需要添加这个包的依赖，不采取。</p>
<h2 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h2><p>　　tools.jar包是JDK自带的，pom.xml中以来的包隐式依赖tools.jar包，而tools.jar并未在库中。找到答案。eclipse启动的jre不是开发用的jre，而你编译报错则是，在eclipse的jre里面没有找到这个包，我们切换eclipse依赖的jre即可。方法：找到eclipse.ini，在-vmargs这个参数前面添加：<br>　　-vm<br>　　C:\Program Files\Java\jdk1.8.0_172\jre\bin\server\jvm.dll<br>　　即可。重启eclipse，maven更新即可。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://skydh.github.io/2019/05/20/大表的全表扫描/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="董航">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="why are you so happy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/20/大表的全表扫描/" itemprop="url">大表的全表扫描</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-20T20:11:31+08:00">
                2019-05-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>　　你的服务器内存2G,你全表查询一个10g的表，内存会不会炸了。</p>
<h2 id="答案"><a href="#答案" class="headerlink" title="答案"></a>答案</h2><p>　　不会。逻辑如下，语句，select * from t；服务端不会存放完整的结果集，其流程和流很类似。<br>　　１.获取一行，写到net_buffer中。这个值由参数net_buffer_length决定，默认16k。<br>　　２.重复获取行，直到，写满net_buffer<br>　　３.若发送成功，就清空net_buffer,然后取下一行，写入搭配net_buffer。<br>　　４.如果发送函数返回EAGAIN,或者WSAEWOULDBLOCK。就表示本地网络栈满了，进入等待，直到可以重写，再继续发送。</p>
<h2 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h2><p>　　-quick：mysql_use_result方法，这个方法是读一行，处理一行。默认的话mysql_store_result这个接口。这个是把结果保存到本地内存。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">董航</p>
              <p class="site-description motion-element" itemprop="description">clear 航</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">111</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          

          <div class="links-of-author motion-element">
            
          </div>

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">董航</span>

  
</div>


<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  本站访客数:<span id="busuanzi_value_site_uv"></span>
</span>
</div>

  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.3</div>




        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
